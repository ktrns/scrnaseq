# 10x feature types and assay names
Assays_10x = list("Gene Expression" = "RNA",
                  "Multiplexing Capture" = "MPLX",
                  "VDJ T" = "VDJT",
                  "VDJ B" = "VDJB",
                  "Antibody Capture" = "ADT",
                  "Peaks" = "ATAC",
                  "Motifs" = "TFMotifs",
                  "CRISPR Guide Capture" = "CRISPR",
                  "Antigen Capture" = "BEAM",
                  "Custom" = "CUSTOM",
                  "Negative Control Codeword" = "ControlCodeword",
                  "Negative Control Probe" = "ControlProbe",
                  "Unassigned Codeword" = "BlankCodeword",
                  "Blank Codeword" = "BlankCodeword")

Assays_Scale = list("Gene Expression" = "RNA",
                  "Antibody Capture" = "ADT",
                  "Chromatin Accessibility" = "ATAC",
                  "CRISPR Guide Capture" = "CRISPR")

Assays_Parse = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC",
                    "CRISPR Guide Capture" = "CRISPR")

Assays_Smartseq = list("Gene Expression" = "RNA",
                    "Antibody Capture" = "ADT",
                    "Chromatin Accessibility" = "ATAC")

#' Reads metadata from an anndata object in hdf5 format (generated by scanpy).
#' 
#' Largely copied and adapted from Azimuth::LoadH5ADobs!
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format
#' @param type Can be 'obs' (for barcodes) or 'var' (for features).
#' @return Metadata (data.frame format)
ReadMetadata_h5ad = function(h5ad_file, type) {
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Open hdf5 file with anndata object
  hdf5_fh = hdf5r::H5File$new(h5ad_file, mode = "r+")
  hd5_data = hdf5_fh[[paste0("/", type)]]
  
  # Create metadata matrix
  index.var = hdf5r::h5attr(hd5_data, "_index")
  index = hd5_data[[index.var]][]
  groups = names(hd5_data)
  matrix = as.data.frame(x = matrix(
    data = NA,
    nrow = length(index),
    ncol = length(groups)
  ))
  colnames(matrix) = groups
  rownames(matrix) = index
  
  # Get columns and values
  if ("__categories" %in% names(x = hd5_data)) {
    hd5_data_cate = hd5_data[["__categories"]]
    for (i in seq_along(groups)) {
      g.i = groups[i]
      value_i = hd5_data[[g.i]][]
      if (g.i %in% names(x = hd5_data_cate)) {
        value_i = factor(x = value_i, labels = hd5_data[[g.i]][])
      }
      matrix[, i] = value_i
    }
  } else {
    for (i in seq_along(groups)) {
      g.i = groups[i]
      if (all(names(hd5_data[[g.i]]) == c("categories", "codes"))) {
        if (length(unique(hd5_data[[g.i]][["codes"]][])) == length(hd5_data[[g.i]][["categories"]][])) {
          value_i = factor(x = hd5_data[[g.i]][["codes"]][], labels = hd5_data[[g.i]][["categories"]][])
        }
        else {
          value_i = hd5_data[[g.i]][["codes"]][]
        }
      }
      else {
        value_i = tryCatch(expr = hd5_data[[g.i]][],
                           error=function(e) return("unknown")
        )
      }
      matrix[, i] = value_i
    }
  }
  
  hdf5_fh$close_all()
  return(as.data.frame(matrix))
}

#' Reads metadata from a character-separated file.
#' 
#' @param h5ad_file Path to a character-separated file. First column must contain the respective barcode or feature id.
#' @return Metadata (data.frame format)
ReadMetadata_csv = function(csv_file) {
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read table
  meta_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatMessage("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata from an Excel file.
#' 
#' @param excel_file Path to an Excel file. First column must contain the respective barcode or feature id.
#' @param sheet Sheet number. description
#' @return Metadata (data.frame format)
ReadMetadata_excel = function(excel_file, sheet=1) {
  # Checks
  assertthat::is.readable(excel_file)
  
  # Read table
  meta_data = readxl::read_excel(excel_file, sheet=sheet, col_names=TRUE)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatMessage("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata from a metadata table saved as R rds file.
#' 
#' This method preserves factor levels.
#' 
#' @param rds_file Path to an R rds file. First column must contain the respective barcode or feature id.
#' @return Metadata (data.frame format)
ReadMetadata_rds = function(rds_file) {
  # Checks
  assertthat::is.readable(rds_file)
  
  # Read table
  meta_data = readRDS(rds_file)
  meta_data = as.data.frame(meta_data)
  
  # Assert that first column is unique
  assertthat::assert_that(all(not(duplicated(meta_data[, 1, drop=TRUE]))),
                          msg=FormatMessage("Metadata file {file} contains duplicate values in first column."))
  
  # Create table
  rownames(meta_data) = meta_data[, 1, drop=TRUE]
  return(meta_data)
}

#' Reads metadata.
#' 
#' @param file Path to a character-separated file (csv, tsv, csv.gz, tsv.gz), an Excel file (xls, xslx) or an R rds file containing a table (preserves factor levels). First column must contain the respective barcode or feature id. For Excel files, a sheet can be specified by appending ':<sheet_number>'.
#' @return Metadata (data.frame format)
ReadMetadata = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatMessage("Metadata file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read metadata
  if (extension %in% c("csv", "tsv")) {
    meta_data = ReadMetadata_csv(file)
  } else if(extension %in% c("xls", "xlsx")) {
    meta_data = ReadMetadata_excel(file, sheet=sheet)
  } else if(extension %in% c("rds")) {
    meta_data = ReadMetadata_rds(file)
  }
  
  # Assert that it is not empty
  assertthat::assert_that(assertthat::not_empty(meta_data),
                          msg=FormatMessage("Metadata file {file} is empty."))

  return(meta_data)
}

#' Reads the datasets table.
#' 
#' TODO: Code may be a bit redundant with ReadMetadata. On the other hand, it does some specific checks.
#' 
#' @param file Path to a character-separated file (csv, tsv, csv.gz, tsv.gz) or an Excel file (xls, xslx). For Excel files, a sheet can be specified by appending ':<sheet_number>'. Needs to contain the following columns:
#' @return Metadata (data.frame format)
ReadDatasetsTable = function(file) {
  # Sheet number appended?
  sheet = 1
  if (grepl(":\\d+$", file)) {
    sheet = gsub(pattern=".+:(\\d+)$", replacement="\\1", x=file)
    file = gsub(pattern=":\\d+$", replacement="", x=file)
  }
  
  # Checks
  extension = tools::file_ext(gsub(pattern="\\.gz$", replacement="", x=file))
  valid_extensions = c("csv", "tsv", "xls", "xlsx", "rds")
  assertthat::assert_that(extension %in% valid_extensions,
                          msg=FormatMessage("Datasets file type must be: {valid_extensions*} (file can be gzipped)."))
  
  # Read datasets table
  if (extension %in% c("csv", "tsv")) {
    datasets_table = readr::read_delim(file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types="text")
  } else if(extension %in% c("xls", "xlsx")) {
    datasets_table = readxl::read_excel(file, sheet=sheet, col_names=TRUE)
  }
  assertthat::assert_that(assertthat::not_empty(datasets_table),
                          msg=FormatMessage("Datasets file {file} is empty."))
  
  # Check that all columns are present
  
  
  return(datasets_table)
}

#' Reads counts from a character-separated file.
#' 
#' @param csv_file Path to a character-separated counts file. First column contains the feature id (barcode id if transpose is set), all other columns contain the barcode (feature) counts.
#' @param transpose If TRUE then rows are barcodes and columns are features (default: FALSE)
#' @return Sparse counts matrix (dgCMatrix format).
ReadCounts_csv = function(csv_file, transpose=FALSE) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(csv_file)
  
  # Read character-separated counts table with gene id in the first column and counts in the other columns 
  counts_data = readr::read_delim(csv_file, col_names=TRUE, comment="#", progress=FALSE, show_col_types=FALSE, col_types=readr::cols())
  row_ids = counts_data[, 1, drop=TRUE]
  col_ids = colnames(counts_data)
  col_ids = col_ids[-1]
  
  # Check that barcodes and features are unique
  assertthat::assert_that(sum(duplicated(col_ids)) == 0,
                          msg=FormatMessage("Dataset {csv_file} contains at least two barcodes with the same name."))
  
  assertthat::assert_that(sum(duplicated(row_ids)) == 0,
                          msg=FormatMessage("Dataset {csv_file} contains at least two features with the same name.")) 
  
  # Check that all columns are numeric
  is_numeric = sapply(counts_data, is.numeric)
  assertthat::assert_that(all(is_numeric[-1]),
                          msg=FormatMessage("There are non-numeric columns in dataset {csv_file}! Only the first column may be non-numeric."))
  
  
  # Create sparse matrix
  if (transpose) {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix() %>% t(), 
                                 sparse=TRUE,
                                 dimnames = list(col_ids, row_ids))
  } else {
    counts_data = Matrix::Matrix(data=counts_data[, -1, drop=FALSE] %>% as.matrix(), 
                                 sparse=TRUE,
                                 dimnames = list(row_ids, col_ids))
  }
  return(list(All=counts_data))
}
  
#' Reads counts that are in market exchange format.
#' 
#' @param mtx_directory Path to counts directory in market exchange format.
#' @param mtx_file_name Name of the matrix mtx file (default: matrix.mtx.gz)
#' @param transpose If TRUE then rows are cells and columns are genes (default: FALSE)
#' @param barcodes_file_name Name of the barcodes character-separated file (default: barcodes.tsv.gz)
#' @param barcodes_column_names How to name the columns in the barcodes file. When TRUE, use the first line as header. When FALSE, use generic names. Alternatively, a character vector with column names can be provided (default: FALSE).
#' @param features_file_name Name of the features character-separated file (default: features.tsv.gz)
#' @param features_column_names How to name the columns in the features file. When TRUE, use the first line as header. When FALSE, use generic names. Alternatively, a character vector with column names can be provided (default: FALSE).
#' @param feature_type_column If there is data for multiple feature types, which column (number) in the features file is used to identify the type. If there is no column, set to NULL and type will be "Gene Expression" (default: NULL)
#' @param delim Delimiter used in barcodes and feature files (default: \t)
#' @param strip_suffix String that needs to be removed from the end of the barcodes (default: NULL) 
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata.
ReadCounts_mtx = function(mtx_directory, mtx_file_name="matrix.mtx.gz", transpose=FALSE, barcodes_file_name="barcodes.tsv.gz", barcodes_column_names=FALSE, features_file_name="features.tsv.gz", features_column_names=FALSE, feature_type_column=NULL, delim="\t", strip_suffix=NULL) {
  # Checks
  for(f in file.path(mtx_directory, c(mtx_file_name, barcodes_file_name, features_file_name))) assertthat::is.readable(f)
  
  # Read market exchange format file
  counts_data = Matrix::readMM(file=file.path(mtx_directory, mtx_file_name))
  if (transpose) {
    counts_data = Matrix::t(counts_data)
  }
  counts_data = as(counts_data, "dgCMatrix")
  
  # Read barcodes file
  barcodes_data = readr::read_delim(file=file.path(mtx_directory, barcodes_file_name), col_names=barcodes_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  barcodes_col_nms = colnames(barcodes_data)
  if (is.logical(barcodes_column_names) && barcodes_column_names==FALSE) {
    barcodes_col_nms = rep("barcode_info", length(barcodes_col_nms)) %>% make.unique(sep="_")

  }
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  barcodes_data = as.data.frame(barcodes_data)
  
  if (!is.null(strip_suffix)) {
    rownames(barcodes_data) = trimws(barcodes_data[, 1, drop=TRUE], which="right", whitespace=strip_suffix)
  } else {
    rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  }
  
  # Read features file
  features_data = readr::read_delim(file=file.path(mtx_directory, features_file_name), col_names=features_column_names, delim=delim, progress=FALSE, show_col_types=FALSE)
  feature_col_nms = colnames(features_data)
  if (is.logical(features_column_names) && features_column_names==FALSE)  {
    feature_col_nms = make.unique(rep("feature_info", length(feature_col_nms)), sep="_")
    
  }
  feature_col_nms[1] = "feature_id"
  colnames(features_data) = feature_col_nms
  features_data = as.data.frame(features_data)
  rownames(features_data) = features_data[, 1, drop=TRUE]
  
  # Split by feature type column
  feature_sets = list("All" = rep(TRUE, nrow(features_data)))
  if (!is.null(feature_type_column)) {
    feature_types = unique(features_data[, feature_type_column, drop=TRUE])
    
    feature_sets = purrr::map(feature_types, function(f) {
      return(features_data[, feature_type_column, drop=TRUE] == f)
    })
    names(feature_sets) = feature_types
  }
  
  # Generate list of matrix (matrices)
  counts_lst = purrr::map(feature_sets, function(f) {
    # Add counts
    cts = counts_data[f, ]
    col_ids = rownames(barcodes_data[, 1, drop=FALSE])
    row_ids = rownames(features_data[f, 1, drop=FALSE])
    dimnames(cts) = list(row_ids, col_ids)
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    return(cts)
  })
  names(counts_lst) = names(counts_lst)
  
  return(counts_lst)
}

#' Reads counts from an anndata object in hdf5 format (generated by scanpy and co).
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' Does not discriminate between feature types.
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format.
#' @return Sparse counts matrix (IterableMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata.
ReadCounts_h5ad = function(h5ad_file) {
  library(magrittr)
  
  # Checks
  assertthat::is.readable(h5ad_file)
  
  # Read barcodes and features data separately
  barcodes_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='obs')
  barcodes_col_nms = colnames(barcodes_data)
  barcodes_col_nms[1] = "orig_barcode"
  colnames(barcodes_data) = barcodes_col_nms
  rownames(barcodes_data) = barcodes_data[, 1, drop=TRUE]
  
  features_data = ReadMetadata_h5ad(h5ad_file=h5ad_file, type='var')
  features_data = features_data %>% dplyr::select(feature_id=gene_id,
                                  feature_name=gene_name,
                                  feature_type=gene_id,
                                  setdiff(colnames(features_data), c("gene_id", "gene_name")))
  features_data$feature_type = NA
  rownames(features_data) = features_data$feature_id
  
  # Read counts and attach barcodes/features data
  counts_data=BPCells::open_matrix_anndata_hdf5(h5ad_file)
  rownames(counts_data) = rownames(features_data)
  attr(counts_data, "barcode_metadata") = barcodes_data
  attr(counts_data, "feature_metadata") = features_data
  
  return(list(All=counts_data))
}


#' Reads counts data produced by plate-based methods like SmartSeq.
#' 
#' @param path Path to counts data. Can be a character-separated file or a matrix exchange format directory (with files matrix.mtx.gz, barcodes.tsv.gz and features.tsv.gz).
#' @param assays This simply sets the assay. Smartseq technologies currently do not support multi-assay datasets.
#' @param version Set to '2' for Smartseq2 or '3' for Smartseq3.
#' @param transpose  If TRUE then rows are cells and columns are genes (default: FALSE)
#' @return A sparse counts matrix (dgCMatrix format)
ReadCounts_SmartSeq = function(path, assays, version, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  assertthat::assert_that(version %in% c("2", "3"),
                          msg=FormatMessage("Smartseq version must be '2' or '3'."))
  
  
  # Convert to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Smartseq), Assays_Smartseq)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(assays %in% valid_assays,
                          msg=FormatMessage("'{assays*} must be: {valid_assays*}."))

  if (dir.exists(path)) {
    # market exchange format
    counts_lst = ReadCounts_mtx(mtx_directory=path,
                   transpose=transpose,
                   mtx_directory="matrix.mtx.gz",
                   barcodes_file_name="barcodes.tsv.gz",
                   barcodes_column_names=FALSE,
                   features_file_name="features.tsv.gz",
                   features_column_names=FALSE,
                   feature_type_column=NULL,
                   delim="\t",
                   strip_suffix=NULL)
    counts_lst = counts_lst[1]
  } else {
    # character-separated file
    counts_lst = ReadCounts_csv(csv_file=path, transpose=transpose)
    barcode_metadata = data.frame(orig_barcode=colnames(counts_lst[[1]]),
                                  row.names=colnames(counts_lst[[1]]))
    feature_metadata = data.frame(feature_id=rownames(counts_lst[[1]]),
                                  row.names=rownames(counts_lst[[1]]))
    attr(counts_lst[[1]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[1]], "feature_metadata") = feature_metadata
  }
  
  # Assay
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  attr(counts_lst[[1]], "technology") = paste0("Smartseq", version)
  attr(counts_lst[[1]], "assay") = assays[1]
  
  return(counts_lst)
}

#' Reads 10x counts that are in market exchange format.
#' 
#' @param mtx_directory Path to 10x counts directory in market exchange format.
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata.
ReadCounts_10x_mtx = function(mtx_directory) {
  # Determine the name of the matrix file
  mtx_file_name = dplyr::case_when(file.exists(file.path(mtx_directory, "matrix.mtx.gz")) ~ "matrix.mtx.gz",
                                   file.exists(file.path(mtx_directory, "matrix.mtx")) ~ "matrix.mtx")
  
  # Determine the name of the barcodes file
  barcodes_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "barcodes.tsv.gz")) ~ "barcodes.tsv.gz",
    file.exists(file.path(mtx_directory, "barcodes.tsv")) ~ "barcodes.tsv"
  )
  
  # Determine the name of the features file
  features_file_name = dplyr::case_when(
    file.exists(file.path(mtx_directory, "features.tsv.gz")) ~ "features.tsv.gz",
    file.exists(file.path(mtx_directory, "features.tsv")) ~ "features.tsv",
    file.exists(file.path(mtx_directory, "genes.tsv")) ~ "genes.tsv",
    file.exists(file.path(mtx_directory, "peaks.bed")) ~ "peaks.bed",
    file.exists(file.path(mtx_directory, "motifs.tsv")) ~ "motifs.tsv"
  )
  
  # Determine the column name of the features file
  if (features_file_name == "peaks.bed") {
    # 10x atac
    features_column_names = c("chr", "start", "end")
  } else if (features_file_name == "motifs.tsv") {
    # 10x atac
    features_column_names = c("tf_full_name", "tf_name")
  } else {
    feature_metadata = readr::read_delim(file.path(mtx_directory, features_file_name), delim="\t", col_names=FALSE, n_max=3, progress=FALSE, show_col_types=FALSE)
    
    if (ncol(feature_metadata) == 6) {
      # 10x multiome
      features_column_names = c("feature_id",
                                "feature_name",
                                "feature_type",
                                "chr",
                                "start",
                                "end")
    } else {
      # 10x other
      features_column_names = c("feature_id", "feature_name", "feature_type")
    }
  }
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=features_column_names,
    feature_type_column=3,
    delim = "\t",
    strip_suffix = "-1"
  )
  
  return(counts_lst)
}

#' Reads 10x counts that are in hdf5 format.
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' @param h5_file Path to a 10x h5 counts file.
#' @return One sparse counts matrix per feature type (IterableMatrix format). Additional information on barcodes and features is attached as attributes. barcode_metadata and feature_metadata.
ReadCounts_10x_h5 = function(h5_file) {
  # Checks
  assertthat::is.readable(h5_file)
  
  # Read barcodes and features data separately
  hdf5_fh = hdf5r::H5File$new(h5_file, mode = "r+")
  
  # No barcodes data, just a vector of the barcodes
  barcodes = hdf5_fh[["matrix/barcodes"]][]
  barcodes_data = data.frame(row.names=trimws(barcodes, which="right", whitespace="-1"), orig_barcode=barcodes)
  
  # Read feature data
  hdf5_features = hdf5_fh[["matrix/features"]]
  non_standard_features = hdf5_features[["_all_tag_keys"]][]
  features_data = data.frame(
    feature_id=hdf5_features[["id"]][],
    feature_name=hdf5_features[["name"]][],
    feature_type=hdf5_features[["feature_type"]][]
  )
  
  if (length(non_standard_features) > 0) {
    non_standard_features_data = purrr::map(non_standard_features, function(f) {
      return(hdf5_features[[f]][])
    })
    names(non_standard_features_data) = non_standard_features
    
    if ("interval" %in% names(non_standard_features_data)) {
      interval_data = list(
        chr = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\1",
          non_standard_features_data[["interval"]]
        ),
        start = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\2",
          non_standard_features_data[["interval"]]
        ),
        end = gsub(
          "^([^:]+):(\\d+)-(\\d+)$",
          "\\3",
          non_standard_features_data[["interval"]]
        )
      )
      
      idx = which(names(non_standard_features_data) == "interval")
      if (idx == 1) {
        pre_idx = c()
      } else {
        pre_idx = 1:(idx - 1)
      }
      if (idx == length(non_standard_features_data)) {
        post_idx = c()
      } else {
        post_idx = (idx + 1):length(non_standard_features_data)
      }
      non_standard_features_data = c(non_standard_features_data[pre_idx],
                                     interval_data,
                                     non_standard_features_data[post_idx])
    }
    features_data = cbind(features_data, non_standard_features_data)
  }
  rownames(features_data) = features_data$feature_id
  hdf5_fh$close_all()
  
  feature_types = unique(features_data$feature_type)
  counts_lst = purrr::map(feature_types, function(f) {
    # Subset counts
    cts = BPCells::open_matrix_10x_hdf5(h5_file, feature_type=f)
    colnames(cts) = trimws(colnames(cts), which="right", whitespace="-1")
    
    # Add barcode metadata
    bc_meta = barcodes_data[colnames(cts), , drop=FALSE]
    attr(cts, "barcode_metadata") = bc_meta
    
    # Add feature metadata
    fc_meta = features_data[features_data$feature_type == f, , drop=FALSE]
    keep = purrr::map_lgl(colnames(fc_meta), function(n) {
      return(all(!is.na(fc_meta[, n, drop=TRUE]) & nchar(fc_meta[, n, drop=TRUE]) > 0))
    })
    attr(cts, "feature_metadata") = fc_meta[, keep, drop=FALSE]
    
    return(cts)
  })
  names(counts_lst) = feature_types
  
  return(counts_lst)
}

#' Reads counts data produced by 10x (non-spatial datasets).
#' 
#' @param path Path to 10x counts data. Can be a 10x hdf5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @return One sparse counts matrix per assay. Format is either IterableMatrix (when reading a h5 file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes and features is attached as attributes.
ReadCounts_10x = function(path, assays=NULL, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_10x), Assays_10x)
  feature_type_to_assay = unlist(Assays_10x)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatMessage("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  if (dir.exists(path)) {
    # 10x market exchange format
    counts_lst = ReadCounts_10x_mtx(mtx_directory=path)
  } else {
    # 10x h5 file
    counts_lst = ReadCounts_10x_h5(h5_file=path)
  }
  
  # 10x Xenium hack: the assay BlankCodeword can be feature type "Unassigned Codeword" (old) and "Blank Codeword" (new)
  if ("BlankCodeword" %in% assays) {
    if ("Unassigned Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Blank Codeword"
    } else if ("Blank Codeword" %in% names(counts_lst)) {
      d = assay_to_feature_type == "Unassigned Codeword"
    }
    assay_to_feature_type = assay_to_feature_type[!d]
  }
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatMessage("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatMessage("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_10x in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "10x"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads counts data produced by 10x Visium.
#' 
#' @param path Path to 10x counts data for 10x Visium. Can be a 10x hdf5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @return One sparse counts matrix per assay. Format is either IterableMatrix (when reading a h5 file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes and features is attached as attributes. Path to a directory with image information is attached as attribute.
ReadCounts_10xVisium = function(path, assays=NULL, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  
  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_visium"
  }
  
  return(counts_lst)
}

#' Reads counts data produced by 10x Xenium.
#' 
#' @param path Path to 10x counts data for 10x Xenium. Can be a 10x h5 file (recommended for big datasets) or a 10x matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @return One sparse counts matrix per assay. Format is either IterableMatrix (when reading a h5 file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes and features is attached as attributes.
ReadCounts_10xXenium = function(path, assays=NULL, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)

  # Read counts
  counts_lst = ReadCounts_10x(path, assays=assays)
  
  # Update technology
  for (i in seq_along(counts_lst)) {
    attr(counts_lst[[i]], "technology") = "10x_xenium"
  }
  
  return(counts_lst)
}

#' Reads Parse Biosciences counts that are in market exchange format.
#' 
#' @param mtx_directory Path to Parse Biosciences counts directory in market exchange format. Typically contains the files count_matrix.mtx, cell_metadata.csv and all_genes.csv.
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata.
ReadCounts_ParseBio_mtx = function(mtx_directory) {
  # Determine the name of the matrix file
  mtx_file_name = "count_matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "cell_metadata.csv"
  
  # Determine the name of the features file
  features_file_name = "all_genes.csv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=TRUE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=TRUE,
    features_file_name=features_file_name,
    features_column_names=TRUE,
    feature_type_column=NULL,
    delim = ","
  )
  
  return(counts_lst)
}

#' Reads Parse Biosciences counts that are in h5ad anndata format.
#' 
#' Note: This function does not read the entire counts data into memory. Instead it returns an iterator object that can be used to
#' retrieve values directly from file (random access). It is recommend to convert this object into a BPcells on-disk storage object.
#' 
#' Does not discriminate between feature types.
#' 
#' @param h5ad_file Path to an anndata object in hdf5 format.
#' @return Sparse counts matrix (IterableMatrix format). Additional information on barcodes and features is attached as attributes barcode_metadata and feature_metadata.
ReadCounts_ParseBio_h5ad = function(h5ad_file) {
  return(ReadCounts_h5ad(h5ad_file))
}

#' Reads counts data produced by Parse Biosciences.
#' 
#' @param path Path to Parse Biosciences counts data. Can be a Parse Biosciences anndata.h5ad file (recommended for big datasets) or a Parse Biosciences matrix exchange format directory.
#' @param assays This simply sets the assay. Parse Bioscience currently does not support multi-assay datasets.
#' @return One sparse counts matrix. Format is either IterableMatrix (when reading an anndata.h5ad file) or dgCMatrix (when reading from a matrix exchange format directory). Additional information on barcodes, features, technology and assays is attached as attributes.
ReadCounts_ParseBio = function(path, assays, transpose=FALSE) {
  # Checks
  assertthat::is.readable(path)
  
  # If assays are specified, check that they are valid
  assay_to_feature_type = setNames(names(Assays_Parse), Assays_Parse)
  valid_assays = names(assay_to_feature_type)
  assertthat::assert_that(all(assays %in% valid_assays),
                          msg=FormatMessage("'{assay} must be: {valid_assays*}."))
  
  # Read counts
  if (dir.exists(path)) {
    # Parse Bio market exchange format
    counts_lst = ReadCounts_ParseBio_mtx(mtx_directory=path)
  } else {
    # Parse Bio anndata h5 file
    counts_lst = ReadCounts_ParseBio_h5ad(h5ad_file=path)
  }
  
  # No multi-assay datasets but keep for now
  #
  # Subset feature types (which correspond to assays)
  #if (!is.null(assays)) {
  #  feature_types = assay_to_feature_type[assays]
  #  f = feature_types %in% names(counts_lst)
  #  assertthat::assert_that(all(f),
  #                          msg=FormatMessage("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
  #  counts_lst = counts_lst[feature_types]
  #}
  
  # Change names from feature type to assay
  #feature_types = names(counts_lst)
  #f = feature_types %in% names(feature_type_to_assay)
  #assertthat::assert_that(all(f),
  #                        msg=FormatMessage("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Parse in functions_io.R."))
  #names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  counts_lst = counts_lst[1]
  names(counts_lst) = assays[1]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "Parse Biosciences"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads Scale Bio counts that are in market exchange format.
#' 
#' @param mtx_directory Path to Scale Bio counts directory in market exchange format. Typically contains the files matrix.mtx.gz, barcodes.tsv.gz and features.tsv.gz.
#' @return One sparse counts matrix per feature type (dgCMatrix format). Additional information on barcodes and features is attached as attributes.
ReadCounts_ScaleBio_mtx = function(mtx_directory) {
  # Determine the name of the matrix file
  mtx_file_name = "matrix.mtx"
  
  # Determine the name of the barcodes file
  barcodes_file_name = "barcodes.tsv"
  
  # Determine the name of the features file
  features_file_name = "features.tsv"
  
  # Use more generic function to data in market exchange format
  counts_lst = ReadCounts_mtx(
    mtx_directory=mtx_directory,
    transpose=FALSE,
    mtx_file_name=mtx_file_name,
    barcodes_file_name=barcodes_file_name,
    barcodes_column_names=FALSE,
    features_file_name=features_file_name,
    features_column_names=c("feature_id", "feature_name", "feature_type"),
    feature_type_column=3,
    delim = "\t"
  )
  
  return(counts_lst)
}

#' Reads counts data produced by Scale Bio
#' 
#' @param path Path to Scale Bio counts data. Must be a Scale Bio matrix exchange format directory.
#' @param assays Which assays to read. If NULL, read all assays.
#' @return  One sparse counts matrix per assay (dgCMatrix format). Additional information on barcodes, features, technology and assay is attached as attributes.
ReadCounts_ScaleBio = function(path, assays) {
  # Checks
  assertthat::is.readable(path)
  
  # Converts assay to feature type in dataset
  assay_to_feature_type = setNames(names(Assays_Scale), Assays_Scale)
  feature_type_to_assay = unlist(Assays_Scale)
  valid_assays = names(assay_to_feature_type)
  
  # If assays are specified, check that they are valid
  if (!is.null(assays)) {
    assertthat::assert_that(all(assays %in% valid_assays),
                            msg=FormatMessage("'{assays} must be: {valid_assays*}."))
  }

  # Read counts
  counts_lst = ReadCounts_ScaleBio_mtx(mtx_directory=path)
  
  # Subset feature types (which correspond to assays)
  if (!is.null(assays)) {
    feature_types = assay_to_feature_type[assays]
    f = feature_types %in% names(counts_lst)
    assertthat::assert_that(all(f),
                            msg=FormatMessage("Dataset {path} does not contain the following types of data: {assays[!f]*} (named {feature_types[!f]*} in the dataset)."))
    counts_lst = counts_lst[feature_types]
  }
  
  # Change names from feature type to assay
  feature_types = names(counts_lst)
  f = feature_types %in% names(feature_type_to_assay)
  assertthat::assert_that(all(f),
                          msg=FormatMessage("Dataset {path} contains a type of data that was not recognized as an assay: {feature_types[!f]*}. Check list Assays_Scale in functions_io.R."))
  names(counts_lst) = feature_type_to_assay[names(counts_lst)]
  
  # Add attributes technology and assay
  for(n in names(counts_lst)) {
    attr(counts_lst[[n]], "technology") = "ScaleBio"
    attr(counts_lst[[n]], "assay") = n
  }
  
  return(counts_lst)
}

#' Reads counts data produced by Smartseq, 10x, 10x Visium, 10x Xenium, Parse Biosciences, Scale Bio.
#' 
#' @param path Path to counts data. Can be: character-separated file (Smartseq), matrix exchange format directory (SmartSeq, 10x, Parse Biosciences, ScaleBio), hdf5 file (10x), h5ad file (Parse Biosciences).
#' @param technology Technology. Can be: 'smartseq2', 'smartseq3', '10x', '10x_visium', '10x_xenium', 'parse' or 'scale'.
#' @param assays If there are multiple assays in the dataset, which assay to read. Multiple assays can be specified. If there is only one assay, this simply sets the assay type.
#' @param barcode_metadata Table with additional barcode metadata. Can also be a list specifying metadata for each assay. First column must contain the barcode. Missing barcodes will be filled with NA.
#' @param feature_metadata Table with additional feature metadata. Can also be a list specifying metadata for each assay. First column must contain the feature id. Missing features will be filled with NA.
#' @param barcode_suffix Suffix to add to the barcodes (default: NULL). When barcodes are renamed, will be applied afterwards.
#' @return  One sparse counts matrix per assay. Format can be dgCMatrix (general) or IterableMatrix (when reading an anndata.h5ad or h5 file). Additional information on barcodes and features is attached as attributes.
ReadCounts = function(path, technology, assays, barcode_metadata=NULL, feature_metadata=NULL, barcode_suffix=NULL) {
  library(magrittr)
  
  #path = "datasets/10x_pbmc_5k_protein/filtered_feature_bc_matrix/"
  #technology = "10x"
  #assays=c("RNA", "ADT")
  #rename_features_metadata=c(2,2)
  #rename_features = NULL
  #on_disk_path="modules/read_data/10x_pbmc_5k_protein"
  #barcode_metadata_file=NULL
  #feature_metadata_file=NULL
  #barcode_suffix=NULL
  #on_disk_overwrite=TRUE
  
  
  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatMessage("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read counts
  if (technology == "smartseq2") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="2")
  } else if (technology == "smartseq3") {
    counts_lst = ReadCounts_SmartSeq(path=path, assays=assays[1], version="3")
  } else if(technology == "10x") {
    counts_lst = ReadCounts_10x(path=path, assays=assays)
  } else if(technology == "10x_visium") {
    counts_lst = ReadCounts_10xVisium(path=path, assays=assays)
  } else if(technology == "10x_xenium") {
    counts_lst = ReadCounts_10xXenium(path=path, assays=assays)
  } else if(technology == "parse") {
    counts_lst = ReadCounts_ParseBio(path=path, assays=assays)
  } else if(technology == "scale") {
    counts_lst = ReadCounts_ScaleBio(path=path, assays=assays)
  }
  
  assertthat::assert_that(assertthat::not_empty(counts_lst),
                          msg=FormatMessage("Count not read counts for dataset {path}, assay {assay}."))
  
  # Add barcode metadata to counts objects
  if (!is.null(barcode_metadata)) {
    assertthat::assert_that(!is(barcode_metadata, "list") | length(barcode_metadata) == length(counts_lst),
                            msg=FormatMessage("Barcode metadata must either be one table or a list of tables for each assay (dataset {path})."))
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other barcode metadata
      if ("barcode_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "barcode_metadata")
      } else {
        metadata = data.frame(id=colnames(counts_lst[[i]]))
      }
      
      if (is(barcode_metadata, "list")) {
        barcode_metadata_assay = barcode_metadata[[i]]
      } else {
        barcode_metadata_assay = barcode_metadata
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(barcode_metadata_assay)[1]
      metadata = dplyr::left_join(x=metadata,
                                   y=barcode_metadata_assay,
                                   by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "barcode_metadata") = metadata
    }
  }

  # Add feature metadata to counts objects
  if (!is.null(feature_metadata)) {
    assertthat::assert_that(!is(feature_metadata, "list") | length(feature_metadata) == length(counts_lst),
                            msg=FormatMessage("Feature metadata must either be one table or a list of tables for each assay (dataset {path})."))
    
    for(i in seq_along(counts_lst)) {
      # Do we have already other feature metadata
      if ("feature_metadata" %in% names(attributes(counts_lst[[i]]))) {
        metadata = attr(counts_lst[[i]], "feature_metadata")
      } else {
        metadata = data.frame(id=rownames(counts_lst[[i]]))
      }
      
      if (is(feature_metadata, "list")) {
        feature_metadata_assay = feature_metadata[[i]]
      } else {
        feature_metadata_assay = feature_metadata
      }
      
      x_id = colnames(metadata)[1]
      x_rownames = rownames(metadata)
      y_id = colnames(feature_metadata_assay)[1]
      metadata = dplyr::left_join(x=metadata,
                                  y=feature_metadata_assay,
                                  by=setNames(y_id, x_id))
      rownames(metadata) = x_rownames
      attr(counts_lst[[i]], "feature_metadata") = metadata
    }
  }
  
  # Make feature names Seurat-compatible (replace '_' with '-') and unique
  for(i in seq_along(counts_lst)) {
    assay = names(counts_lst)[i]
    metadata = attr(counts_lst[[i]], "feature_metadata")
    
    feature_names = rownames(counts_lst[[i]])
    if (any(grepl(pattern="_", x=feature_names, fixed=TRUE))) {
      warning(FormatMessage("Feature names contain '_' for dataset {path}, assay {assay}. All occurences will be replaced with '-'."))
      feature_names = gsub(pattern="_", replacement="-", x=feature_names, fixed=TRUE)
    }
    if (any(duplicated(feature_names))) {
      warning(FormatMessage("Features contains duplicate values for dataset {path}, assay {assay}. Feature names will be made unique."))
      feature_names = make.unique(feature_names)
    }
    
    rownames(counts_lst[[i]]) = feature_names
    rownames(metadata) = feature_names
    attr(counts_lst[[i]], "feature_metadata") = metadata
  }

  # Add barcode suffix
  if (!is.null(barcode_suffix)) {
    for(i in seq_along(counts_lst)) {
      # Counts
      colnames(counts_lst[[i]]) = paste0(colnames(counts_lst[[i]]), barcode_suffix)
      
      # Metadata
      barcode_metadata = attr(counts_lst[[i]], "barcode_metadata")
      rownames(barcode_metadata) = paste0(rownames(barcode_metadata), barcode_suffix)
      attr(counts_lst[[i]], "barcode_metadata") = barcode_metadata
    }
  }
  
  return(counts_lst)
}

#' Write counts data to disk in BPCells matrix directory format.
#' 
#' @param counts A counts matrix. Format can be a standard matrix, a sparse matrix, AnnDataMatrixH5 (when reading anndata.h5ad) or MatrixSubset (when reading hdf5).
#' @param path Where to write counts on disk.
#' @param overwrite Overwrite existing output paths. Default is FALSE.
#' @return An IterableMatrix pointing to the BPCells matrix directory.
WriteCounts_MatrixDir = function(counts, path, overwrite=FALSE) {
  library(BPCells)
  
  # If path exists and overwrite is FALSE, just open the matrix directory
  if (dir.exists(path) & overwrite==FALSE) {
    counts = BPCells::open_matrix_dir(path)
    return(counts)
  }
  
  # Check that the counts object has the correct format
  if (is(counts, "IterableMatrix")) {
    # Produced by BPCells::open_matrix_10x_hdf5 or BPCells::open_matrix_anndata_hdf5
    # Nothing to do
  } else if (is.matrix(counts)) {
    # Standard matrix - convert to sparse matrix
    counts = as(counts, "dgCMatrix")
  } else if(is(counts, "sparseMatrix")) {
    # Sparse matrix - convert to dgCMatrix
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
  }else {
    # No matrix - try to convert to sparse matrix
    counts = as(as.matrix(counts), "dgCMatrix")
  }
  
  # Sparse matrices in dgCMatrix format must be converted to BPCells internal format for better efficiency
  if (is(counts, "dgCMatrix")) {
    counts = as(counts, "IterableMatrix")
  }
  
  # Test if we have non-negative integers, then convert matrix from double to integer to save disk space (default is double)
  vals = as(counts[1:min(1000, nrow(counts)), 1:min(1000, ncol(counts))], "dgCMatrix")
  if (all(vals >= 0) & all(vals == round(vals))) {
    counts = BPCells::convert_matrix_type(counts, type="uint32_t")
  }
  
  # Write to directory
  counts = BPCells::write_matrix_dir(mat=counts, dir=path, overwrite=overwrite)
  
  # Return IterableMatrix pointing to this directory
  return(counts)
}

#' Write counts data to disk in matrix market format.
#' 
#' @param counts A counts matrix. Format can be a standard matrix, a sparse matrix, AnnDataMatrixH5 (when reading anndata.h5ad) or MatrixSubset (when reading hdf5).
#' @param path Where to write counts on disk.
#' @param overwrite Overwrite existing output paths. Default is FALSE.
#' @param metadata If TRUE write barcode and feature metadata. Default is FALSE.
WriteCounts_MatrixMarket = function(counts, path, overwrite=FALSE, metadata=FALSE) {
  if (!dir.exists(path) | overwrite==TRUE) {
    if (!is(counts, "dgCMatrix")) {
      counts = as(counts, "dgCMatrix")
    }
    
    d = file.path(path)
    dir.create(d, showWarnings=FALSE)
    
    mh = file.path(d, "matrix.mtx")
    Matrix::writeMM(counts, file=mh)
    R.utils::gzip(mh, overwrite=TRUE)
    
    bh = gzfile(file.path(d, "barcodes.tsv.gz"), open="wb")
    write(colnames(counts), file=bh)
    close(bh)
    
    fh = gzfile(file.path(d, "features.tsv.gz"), open="wb")
    write.table(assay_feature_meta_data_df, file=fh, row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
    close(fh)
  }
}

#' Reads image data produced by 10x Visium.
#' 
#' @param path Path to the 'spatial' directory produced by 10x Visium.
#' @return A Seurat VisiumV1 object.
ReadImage_10xVisium = function(image_dir) {
  # Checks
  assertthat::is.readable(image_dir)
  for (f in c("tissue_lowres_image.png", "scalefactors_json.json")) {
    assertthat::assert_that(file.exists(file.path(image_dir, f)),
                            msg=FormatMessage("10x Visium image directory {image_dir} misses the file {f}."))
  }
  
  # Read image
  image = Seurat::Read10X_Image(image_dir, filter.matrix=FALSE)
  
  return(image)
}

#' Improved version of SeuratObject::CreateSegmentation.
#' 
#' Further improvements might be possible by parallelisation of purrr.
#' #' 
#' @param coords A coordinate table with columns 'cell', 'x' and 'y'.
#' @return A Seurat Segmentation object.
CreateSegmentationImproved = function(coords) {
  library(sp)
  library(SeuratObject)
  
  assertthat::assert_that(all(colnames(coords) == c("cell", "x", "y")),
                          msg=FormatMessage("Function 'CreateSegmentationImproved' requires a table with columns 'cell', 'x' and 'y'."))
  
  coords_cell_names = coords[[1]]
  coords_cell_names = factor(coords_cell_names, levels=unique(coords_cell_names))
  
  coords = as.matrix(coords[, 2:3])
  coords = split(x=coords, f=coords_cell_names)
  coords = purrr::map(coords, .f=matrix, ncol=2, dimnames=list(NULL, c("x", "y")))
  
  polygons_names = names(coords)
  polygons = purrr::map(seq_along(coords), function(i) {
    return(Polygons(
      srl=list(Polygon(coords=coords[[i]])),
      ID=polygons_names[i])
    )
  })
  polygons = SpatialPolygons(Srl=polygons)
  polygons = as(polygons, "Segmentation")
  CheckGC()
  return(polygons)
}

#' Reads "image data" produced by 10x Xenium.
#' 
#' Note that this is not an actual image but rather a set of pixel coordinates (field of vision = FOV)
#' 
#' @param path Path to the 10x Xenium data directory.
#' @param barcodes If not NULL, character vector of barcodes to subset.
#' @param coordinate_type Load cell "centroids", cell "segmentations" or both (default).
#' @return A Seurat FOV object.
ReadImage_10xXenium = function(image_dir, barcodes=NULL, coordinate_type=c("centroids", "segmentations")) {
  library(data.table)
  
  # Checks
  assertthat::is.readable(image_dir)
  assertthat::assert_that(file.exists(file.path(image_dir, "cells.csv.gz")),
                          msg=FormatMessage("10x Xenium image directory {image_dir} misses the file 'cells.csv.gz'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "cell_boundaries.csv.gz")),
                          msg=FormatMessage("10x Xenium image directory {image_dir} misses the file 'cell_boundaries.csv.gz'."))
  assertthat::assert_that(file.exists(file.path(image_dir, "transcripts.csv.gz")),
                          msg=FormatMessage("10x Xenium image directory {image_dir} misses the file 'transcripts.csv.gz'."))
  
  mols.qv.threshold = 20
  options(stringsAsFactors=FALSE)
  coords = list()
  
  # Read cell centroids and cell area
  cell_centroids = data.table::fread(file.path(image_dir, "cells.csv.gz"), 
                                     stringsAsFactors=FALSE, 
                                     select=c("cell_id", "x_centroid", "y_centroid", "cell_area", "nucleus_area"),
                                     colClasses=c("cell_id"="character", "x_centroid"="numeric", "y_centroid"="numeric", "cell_area"="numeric", "nucleus_area"="numeric"),
                                     col.names=c("cell", "x", "y", "cell_area", "nucleus_area"), 
                                     key="cell",
                                     showProgress=FALSE,
                                     nThread=cores)
  if (!is.null(barcodes)) {
    cell_centroids = cell_centroids[barcodes]
  }
  
  if ("centroids" %in% coordinate_type) {
    coords = c(coords, centroids = SeuratObject::CreateCentroids(cell_centroids[, c("cell", "x", "y")]))
  }
  
  # Read segmentations (area of cells)
  if ("segmentations" %in% coordinate_type) {
    cell_boundaries = data.table::fread(file.path(image_dir, "cell_boundaries.csv.gz"), 
                                        stringsAsFactors=FALSE,
                                        select=c("cell_id", "vertex_x", "vertex_y"),
                                        colClasses=c("cell_id"="character", "vertex_x"="numeric", "vertex_y"="numeric"),
                                        col.names=c("cell", "x", "y"),
                                        key="cell",
                                        showProgress=FALSE,
                                        nThread=cores)
    if (!is.null(barcodes)) {
      cell_boundaries = cell_boundaries[barcodes]
    }
    coords = c(coords, segmentation = CreateSegmentationImproved(cell_boundaries))
  }
  
  
  # Load microns (molecule coordinates)
  transcripts = data.table::fread(file.path(image_dir, "transcripts.csv.gz"), 
                                  stringsAsFactors=FALSE, 
                                  select=c("feature_name", "x_location", "y_location", "qv"),
                                  colClasses=c("feature_name"="character", "x_location"="numeric", "y_location"="numeric", "qv"="numeric"),
                                  col.names=c("gene", "x", "y", "qv"),
                                  key="qv",
                                  showProgress=FALSE,
                                  nThread=cores)
  transcripts = transcripts[qv >= mols.qv.threshold]
  transcripts$qv = NULL
  molecules = SeuratObject::CreateMolecules(transcripts, key='mols_')
  
  # Create FOV (coordinates plus transcript info)
  image = SeuratObject::CreateFOV(coords=coords,
                                  molecules=molecules,
                                  assay = 'Spatial',
                                  key = 'fov_')
  
  # Add information about cell_area and nucleus_area as barcode_metadata
  barcode_metadata = as.data.frame(cell_centroids[, c("cell", "cell_area", "nucleus_area")])
  rownames(barcode_metadata) = as.character(barcode_metadata$cell)
  barcode_metadata$cell = NULL
  attr(image, "barcode_metadata") = barcode_metadata
  
  return(image)
}

#' Reads image data produced by 10x Visium and 10x Xenium.
#' 
#' @param image_dir Path to the 'spatial' directory produced by 10x Visium.
#' @param technology Technology. Can be: 10x_visium', '10x_xenium'.
#' @param assay Default assay for this image.
#' @param barcodes Named vector with barcodes to keep, order and rename. Names are original barcodes and values are barcodes after renaming. Barcodes will be re-ordered.
#' @param coordinate_type For 10x Xenium only: Load cell "centroids", cell "segmentations" or both (default).
#' @return A Seurat VisiumV1 object.
ReadImage = function(image_dir, technology, assay, barcodes=NULL, coordinate_type=c("centroids", "segmentations")) {
  library(magrittr)
  #image_dir = "/group/sequencing/Bfx/scripts/andreasp/scrnaseq/datasets/10x_visium_human_brain_cancer/spatial/"
  #technology = "10x_xenium"
  #barcodes = colnames(counts_lst[[1]][[1]])
  
  # Checks
  valid_technologies = c("10x_visium", "10x_xenium")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatMessage("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read image
  if(technology == "10x_visium") {
    # Visium
    image = ReadImage_10xVisium(image_dir=image_dir)
    
    # Keep only spots that are also present in the assay data (under tissue)
    spots_to_keep = image@coordinates %>% dplyr::filter(tissue==1) %>% row.names()
    image = image[spots_to_keep]
    
    # Subset and re-order
    image = image[names(barcodes) %>% as.character()]
    
    # Rename
    new_barcode_names = barcodes[Seurat::Cells(image) %>% as.character()]
    image = Seurat::RenameCells(image, new.names=new_barcode_names %>% as.character())

  } else if(technology == "10x_xenium") {
    # Xenium
    image = ReadImage_10xXenium(image_dir=image_dir, barcodes=names(barcodes), coordinate_type=coordinate_type)
    
    # Subset and re-order image and metadata
    image = image[names(barcodes) %>% as.character()]
    barcode_metadata = attr(image, "barcode_metadata")
    barcode_metadata = barcode_metadata[names(barcodes),]

    # Rename
    new_barcode_names = barcodes[Seurat::Cells(image) %>% as.character()]
    image = Seurat::RenameCells(image, new.names=new_barcode_names %>% as.character())
    rownames(barcode_metadata) = new_barcode_names
    attr(image, "barcode_metadata") = barcode_metadata
  }
  
  # Set default assay for image
  Seurat::DefaultAssay(image) = assay
  
  return(image)
}

#' Reads summary metrics files produced for SmartSeq data. 
#' 
#' Note: Since there is no generally accepted pipeline for SmartSeq data, a metrics file can contain all kinds of information and 
#' can have any format. Therefore, this function just reads and returns a character-separated table. First column must be the cell name 
#' and all other columns can contain metrics.
#' 
#' @param metrics_file Path to a character-separated metrics file.
#' @return A summary metrics table.
ReadMetrics_Smartseq = function(metrics_file) {
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  
  return(metrics_table)
}

#' Reads summary metrics files produced for 10x, 10x Visium, 10x Xenium data.
#' 
#' @param metrics_file Path to a "metrics_summary.csv" file produced by the 10x pipelines cellranger, spaceranger and xenium ranger.
#' @return One or more tables with summary metrics per library type (only cellranger multi) or a table with summary metrics for the entire 10x experiment (all other 10x pipelines).
ReadMetrics_10x = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/konstantint/bfx2278/cellranger_multi/Pwl3_CMO311_A1_CMO312_alt/outs/per_sample_outs/A1/metrics_summary.csv"
  #metrics_file = "/projects/seq-work/analysis/yuliiah/bfx2322/cellranger_arc/m194T/outs/summary.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)
  metrics_table_colnms = colnames(metrics_table)
  metrics_table = as.data.frame(metrics_table)
  
  if (metrics_table_colnms[1] == "Category") {
    # Produced by cellranger multi
    # https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/metrics-summary-csv
    
    # Subset per-sample metrics
    # Also only keep overall metrics but not the ones calculated for various groups
    metrics_table = metrics_table %>% 
      dplyr::filter(Category=="Cells", is.na(`Grouped By`)) %>%
      dplyr::select(library_type=`Library Type`, Metric=`Metric Name`, Value=`Metric Value`)
    
    # Split by library type and convert into wide table
    metrics_table = split(metrics_table, metrics_table$library_type)
    metrics_table = purrr::map(metrics_table, function(tbl) {
      tbl = tbl %>% dplyr::select(-library_type) %>% tidyr::pivot_wider(names_from="Metric", values_from="Value") %>% as.data.frame()
      return(tbl)
    })
  } else {
    # Produced by other cellranger pipelines (standard, multiome, atac, visium, xenium)
    metrics_table = list(Overall=metrics_table)
  }
  
  return(metrics_table)
}

#' Reads summary metrics files produced for Parse Biosciences data. 
#' 
#' @param metrics_file Path to an "analysis_summary.csv" file produced by the splitpipe pipeline.
#' @return A summary metrics table.
ReadMetrics_ParseBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/L132590_Sl_1_62500cells/BC001/report/analysis_summary.csv"
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=c("Metric", "Value"), col_select=1:2, show_col_types=FALSE, skip=1)
  metrics_table = metrics_table %>% 
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  metrics_table = list(metrics_table)
  
  return(metrics_table)
}

#' Reads summary metrics files produced for Scale Bio data. 
#'
#' @param metrics_file Path to a "reportStatistics.csv" file produced by the ScaleRna pipeline.
#' @return A summary metrics table.
ReadMetrics_ScaleBio = function(metrics_file) {
  #metrics_file = "/projects/seq-work/analysis/SCdev/bfx2279/scalerna/output/reports/csv/BC238.reportStatistics.csv"
  library(magrittr)
  
  # Checks
  assertthat::is.readable(metrics_file)
  
  # Read file
  metrics_table = readr::read_delim(metrics_file, col_names=TRUE, show_col_types=FALSE)

  # Only keep sections "Reads" and "Cells"
  metrics_table = metrics_table %>% dplyr::filter(Sample %in% c("Reads", "Cells"))
  metrics_table = metrics_table %>% 
    dplyr::select(-Sample) %>%
    tidyr::pivot_wider(names_from=1, values_from=2) %>%
    as.data.frame()
  
  metrics_table = list(Overall=metrics_table)
  return(metrics_table)
}

#' Reads summary metrics files produced by Smartseq, 10x, 10x Visium, 10x Xenium, Parse Biosciences, Scale Bio.
#' 
#' @param metrics_file Path to a metrics file.
#' @param technology Technology. Can be: 'smartseq2', 'smartseq3', '10x', '10x_visium', '10x_xenium', 'parse' or 'scale'.
#' @return One or more tables with summary metrics.
ReadMetrics = function(metrics_file, technology) {
  #metrics_file = datasets$metrics_file[1]
  #technology = "10x"
  
  # Checks
  valid_technologies = c("smartseq2", "smartseq3", "10x", "10x_visium", "10x_xenium", "parse", "scale")
  assertthat::assert_that(technology %in% valid_technologies,
                          msg=FormatMessage("Technology is {technology} but must be one of: {valid_technologies*}."))
  
  # Read metrics file
  if (technology %in% c("smartseq2", "smartseq3")) {
    metrics_table = ReadMetrics_SmartSeq(metrics_file=metrics_file)
  } else if(technology %in% c("10x", "10x_visium", "10x_xenium")) {
    metrics_table = ReadMetrics_10x(metrics_file=metrics_file)
  } else if(technology == "parse") {
    metrics_table = ReadMetrics_ParseBio(metrics_file=metrics_file)
  } else if(technology == "scale") {
    metrics_table = ReadMetrics_ScaleBio(metrics_file=metrics_file)
  }
  
  return(metrics_table)
}

#' Parses plate information from the cell names. Mainly used for Smartseq2 datasets where this information is often included in the cell name.
#' 
#' @param cell_names A vector with cell names.
#' @param pattern A regular expression pattern with capture groups for plate number, row or column. Default is '_(\\d+)_([A-Z])(\\d+)$'. If the pattern does not match, all information will be set to NA.
#' @return A data frame with plate information.
ParsePlateInformation = function(cell_names, pattern='_(\\d+)_([A-Z])(\\d+)$') {
  library(magrittr)
  
  # Split cell name into plate information and rest
  rest = gsub(pattern=pattern, replacement="", x=cell_names)
  plate_information = as.data.frame(stringr::str_match(string=cell_names, pattern=pattern), stringsAsFactors=FALSE)
  plate_information[, 1] = NULL
  
  if (ncol(plate_information) == 2) {
    colnames(plate_information) = c("PlateRow", "PlateCol")
    plate_information$PlateNumber = NA 
  } else if (ncol(plate_information) == 3) {
    colnames(plate_information) = c("PlateNumber", "PlateRow", "PlateCol")
  }
  plate_information = plate_information[, c("PlateNumber", "PlateRow", "PlateCol")]
  plate_information$Rest = rest
  
  plate_information$PlateNumber = as.integer(plate_information$PlateNumber)
  plate_information$PlateRow = as.character(plate_information$PlateRow)
  plate_information$PlateCol = as.integer(plate_information$PlateCol)
  
  # Decide on plate layout
  if ("Q" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 24) {
    # super plate?
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:max(c(-Inf,plate_information$PlateCol), na.rm=T), ordered=TRUE)
  } else if ("I" %in% plate_information$PlateRow | max(c(-Inf,plate_information$PlateCol), na.rm=T) > 12) {
    # 384 plate
    plate_information$PlateRow = factor(plate_information$PlateRow, 
                                        levels=c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P"), 
                                        ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, levels=1:24, ordered=TRUE)
  } else {
    plate_information$PlateRow = factor(plate_information$PlateRow, ordered=TRUE)
    plate_information$PlateCol = factor(plate_information$PlateCol, ordered=TRUE)
  }
  
  return(plate_information)
}

#' Saves Seurat object and - if available and requested - associated on-disk layers. Extension of Seurat's SaveSeuratRds.
#' 
#' @param sc A Seurat sc object.
#' @param outdir Output directory for saved Seurat object (sc.rds) and associated on-disk layers. If it does not exist, it will be created.
#' @param on_disk_layers If TRUE also copy existing on-disk layers into this directory.
#' @param clean If there are already files/directories in outdir, remove them.
#' @param relative Make paths to on-disk layers relative.
SaveSeuratRds_Custom = function(sc, outdir, on_disk_layers=TRUE, clean=FALSE, relative=FALSE) {
  library(SeuratObject)
  
  # If output directory does not exist, create it
  if (!dir.exists(outdir)) dir.create(outdir, recursive=TRUE)
    
  # If output directory is not empty, remove all files/directories if clean is set
  if (clean) {
    files = list.files(path=outdir, full.names=TRUE)
    if (length(files) > 0) unlink(files, recursive=TRUE)
  }
  
  # Save Seurat object and on-disk data using the SeuratObject function SaveSeuratRds
  SaveSeuratRds(sc, file=file.path(outdir, "sc.rds"), move=on_disk_layers)
  
  # Then make sure that the paths pointing to the layers are correct
  if (on_disk_layers) {
    sc = readRDS(file.path(outdir, "sc.rds"))
    paths = basename(sc@tools$SaveSeuratRds$path)
    if (!relative) paths = file.path(outdir, paths)
    sc@tools$SaveSeuratRds$path = paths
    
    saveRDS(sc, file=file.path(outdir, "sc.rds"))
  }
}


#' Copies on-disk layers of a Seurat object to a new directory.
#' 
#' @param sc A Seurat sc object.
#' @param dir New directory for on-disk layers.
#' @param assays For which assays should on-disk layers be copied. If NULL, copy all.
#' @param layer For which layers should on-disk layers be copied. If NULL, copy all. Can also be a pattern.
UpdateMatrixDirs = function (sc, dir, assays=NULL, layer=NULL) {
  # New directory for on-disk matrices
  dir = normalizePath(path=dir, winslash="/", mustWork=FALSE)
  if (is.null(assays)) assays = SeuratObject::Assays(sc)
  
  # Stores information about on-disk matrices
  cache = SeuratObject::Tool(sc, slot="SaveSeuratRds")
  
  # Iterate over assays
  progr = progressr::progressor(along=assays)
  progr(message=paste("Copying on-disk matrices to directory", dir), class="sticky", amount=0)
  
  for (a in assays) {
    progr(message = paste("Searching through assay", a), class="sticky", amount=0)
    
    if (!is.null(layer)) {
      layers = SeuratObject::Layers(sc, assay=a, layers=layer)
    } else {
      layers = SeuratObject::Layers(sc, assay=a)
    }
    
    # Iterate over layers
    for(i in seq_along(layers)) {
      # Get IterableMatrix for layer
      data = SeuratObject::LayerData(sc, assay=a, layer=layers[i])
      
      # Get old path
      path = SeuratObject::.FilePath(x=data)
      path = Filter(f=nzchar, x=path)
      if (is.null(path)) next
      
      # Move on-disk matrix directory to new path
      progr(message = paste("Moving layer", layers[i], "to", dir), class="sticky", amount=0)
      new_path = as.character(SeuratObject::.FileMove(path=path, new_path=dir))
      
      # Reload matrix directory with new path into Seurat object
      fnx = SeuratObject::.DiskLoad(data)
      fnx = eval(expr = str2lang(fnx))
      SeuratObject::LayerData(sc, assay=a, layer=layers[i]) = fnx(new_path)
      
      # Update information about on-disk matrices
      if (!is.null(cache)) {
        idx = which(cache$assay == a & cache$layer == layers[i])
        cache$path[idx] = new_path
      }
    }
  }
  progr(type='finish')
  
  # Store information about on-disk matrices
  if (!is.null(cache)) {
    sc@tools$SaveSeuratRds = cache
  }
  
  return(sc)
}
