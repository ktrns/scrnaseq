---
# Module-specific parameters (that are not already in the profile yaml)
params:
  # Name of the module used in configurations
  module: "read_data"
    
  # Relative path to the module directory (which contains the qmd file)
  module_dir: "modules/1_read_data"
  
  # Path to a csv or Excel file containing a table with single-cell datasets. For Excel files, a sheet can be specified by appending ':<sheet_number>'.
  # The table must contain the following columns:
  # - sample (required): Name of the (physical) sample. A sample can have multiple single-cell experiments (e.g. multiple 10x experiments).
  # - experiment (required): Name of the single-cell experiment. A single-cell experiment can have multiple single-cell datasets (e.g. two datasets derived from the same 10x experiment).
  # - technology (required): Single-cell technology used. Can be: "smartseq2" - Smartseq2, "smartseq3" - Smartseq3, "10x" - 10x, "10x_visium" - 10x Visium, 
  #                          "10x_xenium" - 10x Xenium, "parse" - Parse Biosciences, "scale" - ScaleBio.
  # - assays (required): Assay(s) of single-cell data to read. Currently supported are: RNA (Gene Expression), ADT (Antibody Protein Capture) and 
  #                      ATAC (Chromatin Accessibility), CUSTOM (Custom). Multiple assays can be specified comma-separated.
  # - path (required): Path to single-cell dataset. Can be: csv file (Smartseq2/3), mtx matrix market directory (10x, 10x Visium, 10x Xenium, Parse Biosciences,
  #                    ScaleBio), hdf5 file (10x, 10x Visium, 10x Xenium), h5ad file (Parse Biosciences).
  # - metrics_file (can be empty): Path to a file containing a metrics summary table. For Smartseq2/3, this can be any table. For 10x, 10x Visium or 10x Xenium, this 
  #                                must be a "metrics_summary.csv" file produced by the Cellranger pipelines. For Parse Biosciences, this must be a "analysis_summary.csv"
  #                            file produced by the splitpipe pipeline. For ScaleBio, this must be a "*.reportStatistics.csv" file produced by the scalerna pipeline.
  # - barcode_metadata_file (can be empty): Path to a file containing a table of additional barcode metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the barcode. Table
  #                                         does not need to contain all barcodes. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - feature_metadata_file (can be empty): Path to a file containing a table of additional feature metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the feature id. Table
  #                                         does not need to contain all features. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - rename_features (can be empty): When reading counts, ids are used to name features. Features can be renamed by: a) Provide a feature metadata column index or name to rename features, 
  #                                   b) Specify 'ENSEMBL' to fetch gene information from Ensembl (gene features only), c) Leave empty or specify 'NO' to not rename features.
  #                                   If a dataset contains multiple assays, this option can be specified multiple times (comma-separated). 
  # - barcode_suffix (can be empty): Suffix to add to the barcodes. If left empty, numeric suffixes will be added ('-1', '-2', etc). If set to 'NO', no suffixes are added.
  datasets_file: "datasets/10x_pbmc_datasets.xlsx"
  
  # Default assay. All cells must have this assay.
  default_assay: RNA
  
  # For large datasets: Do not keep counts in memory but store on disk in matrix directories. Computations will access only the relevant parts of the data. Once done, matrix directories will be saved together with the Seurat object in the module directory.
  on_disk_counts: true
  
  # For large datasets: Copy matrix directories to a temporary directory for computations. This will improve performance if the temporary directory  has a better performance than normal disks (e.g. SSD). Once done, matrix directories will be copied back to the module directory. The temporary directory will be deleted once the R session exists.
  on_disk_use_tmp: false
  
  # Downsample data to at most n barcodes per sample
  downsample_barcodes_n: null
  
  # Downsample all samples equally according to the smallest sample. Overwritten by downsample_cells_n.
  downsample_barcodes_equally: false
  
  # Downsample data to at most n features per assay
  #   null to deactivate
  downsample_features_n: null
  
  # Parse plate information and add to barcode metadata. 
  parse_plate_information: false
  
  # For parsing plate information, regular expression that captures the plate information part of the name. Plate, row and col will be recorded and the plate information will be removed from the name.
  plate_information_regex: "_(\\d+)_([A-Z])(\\d+)$"
  
  # When parsing plate information, set the name of the single-cell experiment (orig.ident) for each cell to the cell name after having removed the plate information
  plate_information_update_identity: false

  # For 10x Xenium in situ data: Load cell "centroids", cell "segmentations" or both (default).
  spatial_coordinate_type:
    - "centroids"
    - "segmentations"
  
  
# Module execution
execute:
  # Should this module be frozen and never re-rendered?
  # - auto: if the code has not changed (default)
  # - true/false: freeze/re-render
  # Does not apply in interactive mode or when explicitly rendering this document via in rstudio
  freeze: auto
---

# Read data

## Overview

In this chapter, we read in counts data as "features x barcodes" matrices. If provided, we read in additional metadata for features and barcodes. For spatial data, we additionally read in the image data. All data is imported into an Seurat object. Basic summary stats are calculated and shown. Finally, the Seurat object is saved for further processing in the next chapters.

```{r}
#| label: setup
#| message: false
#| warning: false

# If running code interactively in rstudio, set profile here
# When rendering with quarto, profile is already set and should not be overwritten
if (nchar(Sys.getenv("QUARTO_PROFILE")) == 0) {Sys.setenv("QUARTO_PROFILE" = "default")}

# Source general configurations (always)
source("R/general_configuration.R")

# Source required R functions
source("R/functions_util.R")
source("R/functions_io.R")
source("R/functions_plotting.R")
source("R/functions_analysis.R")

# Load libraries
library(knitr)
library(magrittr)
library(gt)
library(Seurat)
library(BPCells)
library(ggplot2)
library(future)

# Get module directory (needed to access files within the module directory)
module_name = params$module_name
module_dir = params$module_dir

# Parallisation plan for all functions that support future
plan(multisession, workers=2)
```

```{r}
#| label: read_data_preparation

# Module directory 'results' contains all output that should be provided together with the final report of the project
dir.create(file.path(module_dir, "results"), showWarnings=FALSE)

# Module directory 'sc' contains the final Seurat object
dir.create(file.path(module_dir, "sc"), showWarnings=FALSE)

# If
on_disk_use_tmp = param("on_disk_use_tmp")
if (on_disk_use_tmp) {
  dir.create(file.path(module_dir, "tmp"), showWarnings=FALSE)
}

```

## Datasets

All required information about the datasets is defined in an Excel table in the file `r param("datasets_file")`. It contains the following columns:

-   `sample`: The name of the biological sample
-   `experiment`: The name of the experiment. A sample can have multiple experiments. If an experiment results in multiple datasets, each dataset is defined in a separate row.
-   `technology`: The single-cell technology with which the dataset was produced. Currently supported are: `smartseq2`, `smartseq3`, `10x`, `10x_visium`, `10x_xenium`, `parse` (Parse Biosciences) and `scale` (Scale Bio).
-   `assays`: One or more assays to read from the dataset.
-   `path`: Path to the dataset file or directory
-   `metrics_file`: A path to a file with run metrics. For `smartseq2` and `smartseq3`, this can be any character-separated table. For all others, this must be the summary file generated by the respective pipeline.
-   `barcode_metadata_files`: File with additional metadata for the barcodes. This metadata will then be available for further analysis.
-   `feature_metadata_files`: File with additional metadata for the features. This metadata will then be available for further analysis.
-   `rename_features`: Whether to rename features based on a metadata column (index) or using `ENSEMBL`
-   `barcode_suffix`: Suffix to make the barcodes unique across experiments

The table of datasets used in this analysis is shown below:

```{r}
#| label: tbl-read_data_datasets
#| tbl-cap: "Datasets to be read"

# Read table with datasets
datasets_file = param("datasets_file")
datasets = ReadDatasetsTable(datasets_file)

#datasets = datasets[1:2,]
#datasets$technology[1] = "parse"
#datasets$assays[1] = "RNA"
#datasets$path[1] = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/gex_combined/BC001/DGE_filtered/"
#datasets$metrics_file[1] = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/gex_combined/BC001/report/analysis_summary.csv"
#datasets$barcode_metadata_files[1] = NA
#datasets$rename_features[1] = "ENSEMBL"
 
#datasets$technology[2] = "parse"
#datasets$assays[2] = "RNA"
#datasets$path[2] = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/gex_combined/BC005/DGE_filtered/"
#datasets$metrics_file[2] = "/projects/seq-work/analysis/annee/bfx2302/splitpipe/gex_combined/BC005/report/analysis_summary.csv"
#datasets$barcode_metadata_files[2] = NA
#datasets$rename_features[2] = "ENSEMBL"



for (i in 1:nrow(datasets)) {
  # Required columns
  assertthat::assert_that(!is.na(datasets$sample[i]),
                            msg=FormatMessage("Column 'sample' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$experiment[i]),
                            msg=FormatMessage("Column 'experiment' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$technology[i]),
                            msg=FormatMessage("Column 'technology' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$assays[i]),
                            msg=FormatMessage("Column 'assays' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$path[i]),
                            msg=FormatMessage("Column 'path' cannot be empty or NA (row {i})."))
  
  # Default assay must be present in all datasets - potential fix: Add an empty default assay table for each dataset without default assay - but is it neccessary (?)
  default_assay = param("default_assay")
  assertthat::assert_that(default_assay %in% (strsplit(datasets$assays[i], ",") %>% unlist() %>% trimws()),
                            msg=FormatMessage("Each dataset must contain the default assay {default_assay} (dataset {path})."))
}

# If barcode_suffix is NA, generates suffixes per experiment (-1, -2, ...) 
if (any(is.na(datasets$barcode_suffix))) {
  assertthat::assert_that(sum(is.na(datasets$barcode_suffix)) == nrow(datasets),
                            msg=FormatMessage("If one barcode suffix is empty/NA, all barcode suffixes need to be empty/NA."))
  datasets$barcode_suffix = factor(datasets$experiment, level=unique(datasets$experiment)) %>% as.integer() %>% paste0("-", .)
}

# Print datasets
gt(datasets)
```

A dataset can have a file with run metrics. This file contains numbers about the general performance of the experiment and is a good starting point to judge the data quality. Here, all available metrics file are read and summarised by technology:

```{r}
#| label: read_data_metrics

# Read metrics tables into list
metrics_tables = purrr::map(1:nrow(datasets), function(i) {
  experiment = datasets$experiment[i]
  technology = datasets$technology[i]
  metrics_file = datasets$metrics_file[i]
  
  if (!is.na(metrics_file)) {
    metrics_table = ReadMetrics(metrics_file=metrics_file,  technology=technology) %>% 
      dplyr::bind_rows() %>%
      dplyr::mutate(Dataset=experiment, .before=1)
  } else {
    metrics_table = data.frame(Dataset=experiment)
  }
  
  return(metrics_table)
})

# Then group and bind rows by technology
dataset_rows_by_tech = split(1:nrow(datasets), datasets$technology)
metrics_tables_by_tech = purrr::map(dataset_rows_by_tech, function(rows) {
  metrics_table = dplyr::bind_rows(metrics_tables[rows])
  return(metrics_table)
})
```

```{r}
#| label: read_data_metric
#| results: asis

# How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
# String to create a chunk for each assay ({{assay}} is placeholder)
chunk_template = "
##### {{technology}}

\`\`\`{r}
#| label: tbl-read_data_metric_{{technology}}
#| tbl-cap: 'Metrics for technology {{technology}}'

metrics_tables_by_tech[['{{technology}}']] %>% gt()
\`\`\`
"

cat("::: panel-tabset", sep="\n")
for(n in names(metrics_tables_by_tech)) {
  chunk_filled = knitr::knit_expand(text=chunk_template, technology=n)
  
  if(interactive()) {
    print(EvalKnitrChunk(chunk_filled))
  } else {
    chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
    cat(chunk_filled, sep='\n')
  }
}
cat(":::", sep="\n")
```

## Read counts

Once datasets are set

```{r}
#| label: read_data_read_counts

# Read counts

############################
# Read counts for datasets #
############################

counts_lst = list()

for (i in 1:nrow(datasets)) {
  ###############################################################################
  # Get experiment, sample, dataset path, technology, assays and barcode suffix #
  ###############################################################################
  sample = datasets$sample[i]
  experiment = datasets$experiment[i]
  path = datasets$path[i]
  technology = datasets$technology[i]
  assays = strsplit(datasets$assays[i], split=",") %>% unlist() %>% trimws()
  barcode_suffix = datasets$barcode_suffix[i]
  if (is.na(barcode_suffix)) {
    barcode_suffix = paste0("-", i)
  } else if (barcode_suffix=='NO') {
    barcode_suffix = NULL
  }

  #########################
  # Read barcode metadata #
  #########################
  barcode_metadata_files = datasets$barcode_metadata_files[i]
  if (is.na(barcode_metadata_files)) {
    barcode_metadata = NULL
  } else {
    barcode_metadata_files = strsplit(barcode_metadata_files, split=",")  %>% unlist() %>% trimws()
    barcode_metadata = purrr::map(barcode_metadata_files, ReadMetadata)
    if (length(barcode_metadata) == 1) {
      barcode_metadata = barcode_metadata[[1]]
    }
  }
  
  #########################
  # Read feature metadata #
  #########################
  feature_metadata_files = datasets$feature_metadata_files[i]
  if (is.na(feature_metadata_files)) {
    feature_metadata = NULL
  } else {
    feature_metadata_files = strsplit(feature_metadata_files, split=",")  %>% unlist() %>% trimws()
    feature_metadata = purrr::map(feature_metadata_files, ReadMetadata)
    if (length(feature_metadata) == 1) {
      feature_metadata = feature_metadata[[1]]
    }
  }
  
  ###############
  # Read counts #
  ###############
  
  # Read counts
  counts_lst[[i]] = ReadCounts(path=path,
                                    technology=technology,
                                    assays=assays,
                                    barcode_metadata=barcode_metadata,
                                    feature_metadata=feature_metadata,
                                    barcode_suffix=barcode_suffix)
  
  #################################
  # If requested, rename features #
  #################################

  rename_features = datasets$rename_features[i]
  if (is.na(rename_features)) {rename_features = 'NO'}
  
  # Rename using a feature metadata column or using Ensembl (keyword: "ENSEMBL")
  if (rename_features != 'NO' ) {
    rename_features = strsplit(rename_features, split=",")  %>% unlist() %>% trimws()
    assertthat::assert_that(length(rename_features) == 1 | length(rename_features) == length(counts_lst[[i]]),
                            msg=FormatMessage("When renaming features, either specify one feature metadata column/ENSEMBL to be used for all assays or specify a feature metadata column/ENSEMBL for each assay (dataset {path})."))
  }
  
  # Make sure there an option for each assay
  if (length(rename_features) == 1) {
      rename_features = rep(rename_features, length(counts_lst[[i]]))
  }
  
  # Now loop through assays and process
  for (j in seq_along(counts_lst[[i]])) {
    assay = attr(counts_lst[[i]][[j]], "assay")
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    
    
    if (rename_features[j] == 'NO') {
      # 'NO': no renaming - just keep the current row names
      new_feature_names = rownames(feature_metadata)
    } else {
  
      # If 'ENSEMBL', get ensembl information for features
      if (rename_features[j] == "ENSEMBL") {
        # Query Ensembl
        ensembl_annotation = EnsemblFetchGeneInfo(ids=rownames(feature_metadata), 
                                                  species=param("species"),
                                                  ensembl_version=param("ensembl"),
                                                  mart_attributes = c(ensembl_id="ensembl_gene_id",ensembl_symbol="external_gene_name",
                                                                      ensembl_biotype="gene_biotype",ensembl_description="description",
                                                                      ensembl_chr="chromosome_name",ensembl_start_position="start_position", 
                                                                      ensembl_end_position="end_position", ensembl_strand="strand"),
                                                  useCache=TRUE)
        feature_metadata = dplyr::bind_cols(feature_metadata, ensembl_annotation, .name_repair="minimal")
        attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
        
        # Use metadata column 'ensembl_symbol'
        rename_features[j] = "ensembl_symbol"
      }
    
      # Check that metadata column exists and get values
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      feature_name_column = rename_features[j]
    
      is_column_index = suppressWarnings(feature_name_column %>% as.numeric() %>% is.na() %>% not())
      if (is_column_index) {feature_name_column = as.numeric(feature_name_column)}
    
      if (is.numeric(feature_name_column)) {
          assertthat::assert_that(feature_name_column <= ncol(feature_metadata),
                                  msg=FormatMessage("Column number {feature_name_column} exceeds the number of columns in the feature metadata for dataset {path}, assay {assay}."))
      } else {
          assertthat::assert_that(feature_name_column %in% colnames(feature_metadata),
                                  msg=FormatMessage("Column {feature_name_column} cannot be found in the feature metadata available for dataset {path}, assay {assay}."))
      }
      new_feature_names = feature_metadata[, feature_name_column, drop=TRUE]
      new_feature_names = ifelse(is.na(new_feature_names), rownames(feature_metadata), new_feature_names)
    }

    
    # Now make new feature names Seurat-compatible
    if (any(grepl(pattern="_", x=new_feature_names, fixed=TRUE))) {
      warning(FormatMessage("New feature names contain '_' after renaming for dataset {path}, assay {assay}. All occurences will be replaced with '-'."))
      new_feature_names = gsub(pattern="_", replacement="-", x=new_feature_names, fixed=TRUE)
    }
    
    # Keep new names in feature metadata column 'feature_name'
    feature_metadata[["feature_name"]] = new_feature_names
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
  }

  ##################
  # Other metadata #
  ##################
  for (j in seq_along(counts_lst[[i]])) {
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}

##################################################
# Now rename features in each assay              #
# but make sure it is consistent across datasets #
##################################################

# Get rownames (old names) and feature metadata column feature_name (new names)
features_renaming = purrr::map_depth(counts_lst, 2, function(cts) {
  metadata = attr(cts, "feature_metadata") %>% 
    dplyr::select(new_name=feature_name) %>%
    tibble::rownames_to_column("old_name")
  metadata$assay = attr(cts, "assay")
  return(metadata)
}) %>% purrr::flatten() %>% dplyr::bind_rows() %>% unique()

features_renaming = split(features_renaming, features_renaming$assay)
features_renaming = purrr::map(features_renaming, function(df) {
  df$assay = NULL
  df = unique(df) %>% dplyr::arrange(old_name)
  df$new_name = make.unique(df$new_name)
  df = setNames(df$new_name, df$old_name)
  return(df)
})

# Rename
for (i in seq(counts_lst)) {
  for (j in seq(counts_lst[[i]])) {
    # Metadata gets lost when renaming
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
    technology = attr(counts_lst[[i]][[j]], "technology")
    assay = attr(counts_lst[[i]][[j]], "assay")
    sample = attr(counts_lst[[i]][[j]], "sample")
    experiment = attr(counts_lst[[i]][[j]], "orig.ident")
    path = attr(counts_lst[[i]][[j]], "path")
    
    # Rename
    old_names = rownames(counts_lst[[i]][[j]]) %>% as.character()
    new_names = features_renaming[[assay]][old_names]
    rownames(counts_lst[[i]][[j]]) = new_names
    rownames(feature_metadata) = new_names
    
    # Restore metadata
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]][[j]], "technology") = technology
    attr(counts_lst[[i]][[j]], "assay") = assay
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}

######################################################
# If requested, write counts to matrix directory     #
# else convert to in-memory matrix of type dgCMatrix #
######################################################

# - allows to analyse big datasets
# - requires Seurat v5 and BPcells

on_disk_counts = param("on_disk_counts")

for (i in seq(counts_lst)) {
  for(j in seq_along(counts_lst[[i]])) {
    # Metadata gets lost when saving
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
    technology = attr(counts_lst[[i]][[j]], "technology")
    assay = attr(counts_lst[[i]][[j]], "assay")
    sample = attr(counts_lst[[i]][[j]], "sample")
    experiment = attr(counts_lst[[i]][[j]], "orig.ident")
    path = attr(counts_lst[[i]][[j]], "path")
  
    if (on_disk_counts) {
      # Write counts to matrix directory
      WriteCounts_MatrixDir(counts=counts_lst[[i]][[j]], 
                  path=file.path(module_dir, "tmp", paste(experiment, assay, "counts", sep=".")), 
                  overwrite=param("on_disk_counts_overwrite"))
      
      # Then open matrix directory for analysis
      counts_lst[[i]][[j]] = BPCells::open_matrix_dir(file.path(module_dir, "tmp", paste(experiment, assay, "counts", sep=".")))
    } else {
      # Convert to in-memory matrix
      counts_lst[[i]][[j]] = as(counts_lst[[i]][[j]], "dgCMatrix")
    }
    
    # Add attributes barcode_metadata and feature_metadata, as well as technology and assay
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]][[j]], "technology") = technology
    attr(counts_lst[[i]][[j]], "assay") = assay
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}
```

Here is the summary of barcodes and features for each dataset:

```{r}
#| label: tbl-read_data_counts_summary
#| tbl-cap: "Counts for each dataset"

# Print summary of feature and barcodes
counts_summary = data.frame(
  sample = purrr::map_depth(counts_lst, 2, attr, "sample") %>% purrr::flatten() %>% unlist(),
  dataset = purrr::map_depth(counts_lst, 2, attr, "orig.ident") %>% purrr::flatten() %>% unlist(),
  assay = purrr::map_depth(counts_lst, 2, attr, "assay") %>% purrr::flatten() %>% unlist(),
  barcodes = purrr::map_depth(counts_lst, 2, ncol) %>% purrr::flatten() %>% unlist(),
  features = purrr::map_depth(counts_lst, 2, nrow) %>% purrr::flatten() %>% unlist())

gt(counts_summary)
```

In case, the counts datasets are very big, it is possible to (randomly) sample barcodes to generate smaller counts subsets which may be faster to analyse but still give meaningful results. The number of barcodes to sample can either be a fix number or the number of barcodes in the smallest sample. Similarly, it also possible to sample features down to a fix number.

```{r}
#| label: read_data_downsample

# Downsample barcodes

# Determine number of barcodes to downsample
downsample_barcodes_n = param("downsample_barcodes_n")
downsample_barcodes_equally = param("downsample_barcodes_equally")

if (!is.null(downsample_barcodes_n) | downsample_barcodes_equally) {
  # Get barcodes per dataset
  barcodes_per_dataset = purrr::map(counts_lst, function(cts) {
    purrr::map(cts, colnames) %>% 
      unlist %>%
      unique() %>%
      return()
  })
  
  # If downsample equally, update sample number to number of barcodes of the smallest sample
  if (downsample_barcodes_equally) {
    downsample_barcodes_n = purrr::map(barcodes_per_dataset, length) %>% 
      unlist() %>% 
      min()
  }
  
  # Downsample barcodes
  barcodes_per_dataset = purrr::map(barcodes_per_dataset, function(b) {
    set.seed(getOption("random_seed"))
    b = sample(b, downsample_barcodes_n)
    return(b)
  })
  
  # Subset counts and metadata
  for (i in seq(counts_lst)) {
    for(j in seq_along(counts_lst[[i]])) {
      # Metadata gets lost when subsetting
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
      technology = attr(counts_lst[[i]][[j]], "technology")
      assay = attr(counts_lst[[i]][[j]], "assay")
      sample = attr(counts_lst[[i]][[j]], "sample")
      experiment = attr(counts_lst[[i]][[j]], "orig.ident")
      path = attr(counts_lst[[i]][[j]], "path")
      
      # Subset counts
      keep = colnames(counts_lst[[i]][[j]]) %in% barcodes_per_dataset[[i]]
      counts_lst[[i]][[j]] = counts_lst[[i]][[j]][, keep]
      
      # Subset barcode metadata
      keep = rownames(barcode_metadata) %in% barcodes_per_dataset[[i]]
      barcode_metadata = barcode_metadata[keep,]

      # Restore metadata
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]][[j]], "technology") = technology
      attr(counts_lst[[i]][[j]], "assay") = assay
      attr(counts_lst[[i]][[j]], "sample") = sample
      attr(counts_lst[[i]][[j]], "orig.ident") = experiment
      attr(counts_lst[[i]][[j]], "path") = path
    }
  }
}

# Downsample features
downsample_features_n = param("downsample_features_n")

if (!is.null(downsample_features_n)) {
  
    # Subset counts and metadata
  for (i in seq(counts_lst)) {
    for(j in seq_along(counts_lst[[i]])) {
      # Metadata gets lost when subsetting
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
      technology = attr(counts_lst[[i]][[j]], "technology")
      assay = attr(counts_lst[[i]][[j]], "assay")
      sample = attr(counts_lst[[i]][[j]], "sample")
      experiment = attr(counts_lst[[i]][[j]], "orig.ident")
      path = attr(counts_lst[[i]][[j]], "path")
      
      # Sample row indices
      set.seed(getOption("random_seed"))
      rows = sample(seq(nrow(counts_lst[[i]][[j]])), downsample_features_n)
      
      # Subset counts
      counts_lst[[i]][[j]] = counts_lst[[i]][[j]][rows, ]
      
      # Subset feature metadata
      feature_metadata = feature_metadata[rows,]

      # Restore metadata
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]][[j]], "technology") = technology
      attr(counts_lst[[i]][[j]], "assay") = assay
      attr(counts_lst[[i]][[j]], "sample") = sample
      attr(counts_lst[[i]][[j]], "orig.ident") = experiment
      attr(counts_lst[[i]][[j]], "path") = path
    }
  }
}

  
# Message
if (!is.null(downsample_barcodes_n) & !is.null(downsample_features_n)) {
  glue::glue("\n::: {{.callout-note}}\nYour data has been downsampled to at most {downsample_barcodes_n} barcodes and at most {downsample_features_n} features!\n:::\n") %>% knitr::asis_output()
} else if (!is.null(downsample_barcodes_n)) {
  glue::glue("\n::: {{.callout-note}}\nYour data has been downsampled to at most {downsample_barcodes_n} barcodes!\n:::\n") %>% knitr::asis_output()
} else if (!is.null(downsample_barcodes_n)) {
  glue::glue("\n::: {{.callout-note}}\nYour data has been downsampled to at most {downsample_features_n} features!\n:::\n") %>% knitr::asis_output()
}
```

## Create Seurat object

Create object

```{r}
#| label: read_data_seurat_object
#| timeit: true

# List all assays in all datasets
all_assays = purrr::map_depth(counts_lst, 2, attr, "assay") %>% 
  purrr::flatten() %>% 
  unlist() %>% 
  unique()

#################
# Default assay #
#################

# Collect default assay datasets
default_assay = param("default_assay")
has_assay = purrr::map(counts_lst, purrr::pluck_exists, default_assay) %>% purrr::flatten_lgl()
assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, default_assay)
names(assay_counts_lst) = purrr::map(assay_counts_lst, attr, "orig.ident") %>% unlist()

# Create Seurat default assay object
assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
if (length(assay_counts_lst) == 1) {
  n = names(assay_counts_lst)
  assay_obj = split(assay_obj, rep(n, ncol(assay_obj)))
}
rownames(assay_obj@meta.data) = rownames(assay_obj)

# Collect feature metadata for default assay datasets
feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
  metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
  return(metadata)
}) %>% dplyr::bind_rows()

# Group by feature name and make unique values
feature_metadata = feature_metadata %>% 
  dplyr::group_by(rowname) %>%
  dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
  as.data.frame()
rownames(feature_metadata) = feature_metadata$rowname
feature_metadata = feature_metadata %>% dplyr::select(-rowname)

# Add to Seurat default assay object
assay_obj = SeuratObject::AddMetaData(assay_obj, metadata=feature_metadata)

# Collect barcode metadata (from all assays/datasets)
barcode_metadata = purrr::map_dfr(seq_along(counts_lst), function(i) {
  metadata = data.frame(rowname=colnames(assay_counts_lst[[i]]))
  metadata$orig.ident = attr(assay_counts_lst[[i]], "orig.ident")
  metadata$sample = attr(assay_counts_lst[[i]], "sample")
  metadata$technology = attr(assay_counts_lst[[i]], "technology")
  
  bc_meta = attr(assay_counts_lst[[i]], "barcode_metadata") %>% 
    tibble::rownames_to_column()
  bc_meta$sample = NULL
  bc_meta$technology = NULL
  bc_meta$orig.ident = NULL
  metadata = dplyr::left_join(metadata, bc_meta, by="rowname")
  rownames(metadata) = metadata$rowname
  
  metadata = metadata %>% dplyr::select(-rowname)
  return(metadata)
})
barcode_metadata$orig.ident = factor(barcode_metadata$orig.ident, levels=unique(barcode_metadata$orig.ident))
barcode_metadata$sample = factor(barcode_metadata$sample, levels=unique(barcode_metadata$sample))
barcode_metadata$technology = factor(barcode_metadata$technology, levels=unique(barcode_metadata$technology))

# Create Seurat object
sc = Seurat::CreateSeuratObject(counts=assay_obj, meta.data=barcode_metadata, assay=default_assay, names.delim=NULL, names.field=NULL)

# Fix NA values in barcode metadata
n =  paste("nCount", default_assay, sep="_")
i = is.na(sc[[]][, n]) %>% which()
sc[[]][i, n] = 0

n =  paste("nFeature", default_assay, sep="_")
i = is.na(sc[[]][, n]) %>% which()
sc[[]][i, n] = 0

################
# Other assays #
################

# Add other assays to Seurat object
for(a in setdiff(all_assays, default_assay)) {
  # Collect assay datasets
  has_assay = purrr::map(counts_lst, purrr::pluck_exists, a) %>% purrr::flatten_lgl()
  assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, a)
  names(assay_counts_lst) = purrr::map(assay_counts_lst, attr, "orig.ident") %>% unlist()

  # Create Seurat default assay object
  assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
  if (length(assay_counts_lst) == 1) {
    n = names(assay_counts_lst)
    assay_obj = split(assay_obj, rep(n, ncol(assay_obj)))
  }
  rownames(assay_obj@meta.data) = rownames(assay_obj)

  # Collect feature metadata for default assay datasets
  feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
    metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
    return(metadata)
  }) %>% dplyr::bind_rows()

  # Group by feature name and make unique values
  feature_metadata = feature_metadata %>% 
    dplyr::group_by(rowname) %>%
    dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
    as.data.frame()
  rownames(feature_metadata) = feature_metadata$rowname
  feature_metadata = feature_metadata %>% dplyr::select(-rowname)

  # Add to Seurat default assay object
  assay_obj = SeuratObject::AddMetaData(assay_obj, metadata=feature_metadata)
  sc[[a]] = assay_obj
  
  # Fix NA values in barcode metadata
  n =  paste("nCount", a, sep="_")
  i = is.na(sc[[]][, n]) %>% which()
  sc[[]][i, n] = 0
  
  n =  paste("nFeature", a, sep="_")
  i = is.na(sc[[]][, n]) %>% which()
  sc[[]][i, n] = 0
  
}

#########
# Other #
#########

# Parse plate information if requested
parse_plate_information = param("parse_plate_information")
plate_information_regex = param("plate_information_regex")
plate_information_update_identity = param("plate_information_update_identity")
  
if (parse_plate_information) {
  # Parse plate information
  plate_information = ParsePlateInformation(sc$orig_barcode, pattern=plate_information_regex)
  rownames(plate_information) = colnames(sc)
  rest = plate_information$Rest
  plate_information$Rest = NULL
  
  # Add to seurat metadata
  sc = Seurat::AddMetaData(sc, plate_information)
  
  # If requested, update cell identity (name of the single-cell experiment) - but only for cells where plate information was parsed
  if (plate_information_update_identity) {
    have_plate_information = !is.na(plate_information$PlateRow) & !is.na(plate_information$PlateCol)
    sc$orig.ident = ifelse(have_plate_information, rest, as.character(sc$orig.ident))
    sc$orig.ident = factor(sc$orig.ident, levels=unique(sc$orig.ident))
  }
}

# Set default identity
Seurat::Idents(sc) = "orig.ident"

# Add colours
sc = ScAddColours(sc, colours=list(
  orig.ident=setNames(zeileis_28[1:length(levels(sc$orig.ident))], levels(sc$orig.ident)),
  sample=setNames(zeileis_28[1:length(levels(sc$sample))], levels(sc$sample)),
  technology=setNames(zeileis_28[1:length(levels(sc$technology))], levels(sc$technology))
))
```

Add images

```{r}
#| label: read_data_read_images
#| timeit: true

# Add images (10x Visium and 10x Xenium only)
for (i in seq_along(counts_lst)) {
  technology = attr(counts_lst[[i]][[1]], "technology")
  orig_ident = attr(counts_lst[[i]][[1]], "orig.ident")
  path = attr(counts_lst[[i]][[1]], "path")
  default_assay = param("default_assay")
  
  # Two technologies have image data
  if (technology == "10x_visium") {
    # 10x Visium: image is in subdirectory 'spatial'
    image_dir = file.path(dirname(path), "spatial")
    assertthat::assert_that(dir.exists(image_dir),
                                msg=FormatMessage("10x Visium dataset {path} does not have a directory 'spatial' with spatial data."))
  } else if (technology == "10x_xenium") {
    # 10x Xenium_ image is in the same directory
    image_dir = dirname(path)
  } else {
    # other: skip
    next
  }
  
  # For renaming and reordering of barcodes in image
  barcodes = sc[[]] %>%
    dplyr::filter(orig.ident==orig_ident) %>%
    dplyr::select(orig_barcode)
  barcodes = setNames(rownames(barcodes), barcodes$orig_barcode)
  j = match(barcodes, Cells(sc))
  barcodes = barcodes[order(j)]
  
  # Read image
  spatial_coordinate_type = param("spatial_coordinate_type")
  image = ReadImage(image_dir=image_dir, 
                    technology=technology, 
                    assay=default_assay, 
                    barcodes=barcodes,
                    coordinate_type=spatial_coordinate_type)
  
  # Add image (10x visium) or field of view (10x xenium)
  if (technology == "10x_visium") {
    key = "image"
  } else if (technology == "10x_xenium") {
    key = "fov"
  }
  key = paste0(key, "." , orig_ident)
  image@key = paste0(gsub("[^[:alnum:]]", "", key), "_")
  sc[[key]] = image
  
  # If additional barcode metadata is provided, add as well
  barcode_metadata = attr(image, "barcode_metadata")
  if (!is.null(barcode_metadata)) {
    sc = Seurat::AddMetaData(sc, barcode_metadata)
  }
  
}
```

```{r}
#| label: read_data_basic_stats

############
# Barcodes #
############
# Calculate for each assay
for (a in Seurat::Assays(sc)) {

  m = paste("pCountsTop50", a, sep="_")
  sc[[m]] = 0

  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]], "counts")) {
    # Get data for layer (sample)
    counts = sc[[a]][l]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]

    # Get counts of the top 50 features per barcode
    with_progress({
      top50_counts = SumTopN(counts, n=50, margin=2, chunk_size=5000)
    })
    
    # Zero counts barcodes will not be in top50_counts - add them here
    top50_perc = ifelse(total_counts>0, top50_counts/total_counts*100, NA)
    
    # Then calculate percentage top50
    top50_perc = setNames(top50_perc, bcs)
    m = paste("pCountsTop50", a, sep="_")
    sc[[m]][names(top50_perc), ] = top50_perc
  }
}

############
# Features #
############

# Calculate for each assay
for (a in Seurat::Assays(sc)) {
  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]], "counts")) {
    # Get data for layer (sample)
    counts = sc[[a]][l]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]
    counts_perc = t(t(counts)/total_counts)*100
    
    # Calculate feature filters
    num_bcs_expr = rowSums(counts >= 1)

    # Calculate median percentage of counts per feature
    with_progress({
      counts_median = CalculateMedians(counts_perc, margin=1, chunk_size=1000)
    })
    
    # Add to metadata
    n = gsub(x=l, pattern="counts\\.", replacement="")
    sc[[a]][[paste("nBcs", n, sep="_")]] = num_bcs_expr
    sc[[a]][[paste("medianPerc", n, sep="_")]] = counts_median
  }
}
```

The following table shows available metadata (columns) of the first five barcodes (rows). These metadata provide additional information such as:

-   the dataset it belongs to ("orig.ident")
-   the sample it belongs to
-   the technology with which it was produced
-   its original name in the dataset
-   additional metadata provided together with it
-   the total number of counts and features for each assay

```{r}
#| label: tbl-read_data_barcode_metadata
#| tbl-cap: "Barcode metadata"

df = sc[[]] %>% head(5)
gt(df, rownames_to_stub=TRUE)
```

The next table shows available metadata (columns) of the first five features (rows) for each assay.

```{r}
#| label: read_data_feature_metadata
#| results: asis

# How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
# String to create a chunk for each assay ({{assay}} is placeholder)
chunk_template = "
  
##### {{assay}}

\`\`\`{r}
#| label: tbl-read_data_feature_metadata_{{assay}}
#| tbl-cap: 'Feature metadata for assay {{assay}}'

sc[['{{assay}}']][[]] %>% head(5) %>% gt(rownames_to_stub=TRUE)
\`\`\`

"

cat("::: panel-tabset", sep="\n")
for (a in Seurat::Assays(sc)) {
  chunk_filled =  knitr::knit_expand(text=chunk_template, assay=a)
  
  if(interactive()) {
    print(EvalKnitrChunk(chunk_filled))
  } else {
    chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
    cat(chunk_filled, sep='\n')
  }
}
cat(":::", sep="\n")
```

## Basic summary

```{r}
#| label: read_data_basic_summary
#| results: asis

orig_idents = levels(sc$orig.ident)

chunk_template = "
\`\`\`{r}
#| label: fig-read_data_basic_summary_{{assay}}_{{i}}
#| fig-asp: 0.5
#| fig-cap: '{{caption}}'
#| echo: false
#| warning: false

print(plist[[{{i}}]])
\`\`\`
"

# Do this per assay
for (a in Seurat::Assays(sc)) {
  cat("### ", a, "\n\n")
  
  # Generate barcode plots
  barcode_qc = paste(c("nCount", "nFeature", "pCountsTop50"), a, sep="_") 
  plist = PlotBarcodeQC(sc, qc=barcode_qc)

  # Prepare captions
  captions = GeneratePlotCaptions(names(plist), remove=paste0("_", a))
  
  # Set up layout and generate chunks
  if (length(orig_idents) <= 10) {
    cat("::: {layout-ncol=2}", sep="\n")
  } else {
    cat("::: {layout-ncol=1}", sep="\n")
  }
  
  for (i in seq(plist)) {
    chunk_filled =  knitr::knit_expand(text=chunk_template, assay=a, i=i, caption=captions[[i]])
    if(interactive()) {
      EvalKnitrChunk(chunk_filled)
    } else {
      chunk_filled = knitr::knit_child(text=chunk_filled, envir=environment(), quiet=TRUE)
      cat(chunk_filled, '\n')
    }
  }
  cat(":::", sep="\n")
}
```

## Save Seurat object

```{r}
#| label: read_data_save_seurat

# Save Seurat object and layer data
with_progress({
  SaveSeuratRds_Custom(sc,
                outdir=file.path(module_dir, "sc"),
                clean=TRUE)
})

# Clean tmp directory
tmp_files = list.files(file.path(module_dir, "tmp"), full.names=TRUE)
for(t in tmp_files) {
  unlink(t, recursive=TRUE)
}
```
