---
# Module title
title: Read data

# Module-specific parameters (that are not already in the profile yaml)
params:
  # Name of the module. Must be the same as name of qmd file.
  module: "1_read_data"
  
  # Path to a csv or Excel file containing a table with single-cell datasets. For Excel files, a sheet can be specified by appending ':<sheet_number>'.
  # The table must contain the following columns:
  # - sample (required): Name of the (physical) sample. A sample can have multiple single-cell experiments (e.g. multiple 10x experiments).
  # - experiment (required): Name of the single-cell experiment. A single-cell experiment can have multiple single-cell datasets (e.g. two datasets derived from the same 10x experiment).
  # - technology (required): Single-cell technology used. Can be: "smartseq2" - Smartseq2, "smartseq3" - Smartseq3, "10x" - 10x, "10x_visium" - 10x Visium, 
  #                          "10x_xenium" - 10x Xenium, "parse" - Parse Biosciences, "scale" - ScaleBio.
  # - assays (required): Assay(s) of single-cell data to read. Currently supported are: RNA (Gene Expression), ADT (Antibody Protein Capture) and 
  #                      ATAC (Chromatin Accessibility), CUSTOM (Custom). Multiple assays can be specified comma-separated.
  # - path (required): Path to single-cell dataset. Can be: csv file (Smartseq2/3), mtx matrix market directory (10x, 10x Visium, 10x Xenium, Parse Biosciences,
  #                    ScaleBio), hdf5 file (10x, 10x Visium, 10x Xenium), h5ad file (Parse Biosciences).
  # - metrics_file (can be empty): Path to a file containing a metrics summary table. For Smartseq2/3, this can be any table. For 10x, 10x Visium or 10x Xenium, this 
  #                                must be a "metrics_summary.csv" file produced by the Cellranger pipelines. For Parse Biosciences, this must be a "analysis_summary.csv"
  #                            file produced by the splitpipe pipeline. For ScaleBio, this must be a "*.reportStatistics.csv" file produced by the scalerna pipeline.
  # - barcode_metadata_file (can be empty): Path to a file containing a table of additional barcode metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the barcode. Table
  #                                         does not need to contain all barcodes. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - feature_metadata_file (can be empty): Path to a file containing a table of additional feature metadata. Can be: a csv, Excel or an R table saved as rds file. 
  #                                         For Excel files, a sheet can be specified by appending ':<sheet_number>'. First column must contain the feature id. Table
  #                                         does not need to contain all features. If a dataset contains multiple assays, multiple files can be provided (comma-separated).
  # - rename_features (can be empty): When reading counts, ids are used to name features. Features can be renamed by: a) Provide a feature metadata column index or name to rename features, 
  #                                   b) Specify 'ENSEMBL' to fetch gene information from Ensembl (gene features only), c) Leave empty or specify 'NO' to not rename features.
  #                                   If a dataset contains multiple assays, this option can be specified multiple times (comma-separated). 
  # - barcode_suffix (can be empty): Suffix to add to the barcodes. If left empty, numeric suffixes will be added ('-1', '-2', etc). If set to 'NO', no suffixes are added.
  datasets_file: "datasets/10x_pbmc_datasets.xlsx"
  
  # Default assay. All cells must have this assay.
  default_assay: RNA
  
  # For large datasets: Do not keep counts in memory but store on disk in matrix directories. Computations will access only the relevant parts of the data. TThis will create a a 'counts' directory within the module directory with one matrix directory per sample and assay.
  on_disk_counts: true
  
  # For large datasets: Overwrite existing matrix directories. The counts files will still be read though.
  on_disk_counts_overwrite: true
  
  # Downsample data to at most n barcodes per sample
  #   null to deactivate
  downsample_barcodes_n: null
  
  # Downsample all samples equally according to the smallest sample
  #   true/false
  #   Overwritten by downsample_cells_n
  downsample_barcodes_equally: false
  
# Module execution
execute:
  # Should this module be re-rendered?
  # - auto: only when code changes
  # - true/false: always/never
  # Does not apply in interactive mode or when explicitly rendering this document via in rstudio
  freeze: auto
---

```{r}
#| label: setup
#| message: false
#| warning: false

# If running code interactively in rstudio, set profile here
# When rendering with quarto, profile is already set and should not be overwritten
if (nchar(Sys.getenv("QUARTO_PROFILE")) == 0) {Sys.setenv("QUARTO_PROFILE" = "default")}

# Source general configurations (always)
source("R/general_configuration.R")

# Source required R functions
source("R/functions_util.R")
source("R/functions_io.R")

# Load libraries
library(knitr)
library(magrittr)
library(gt)
library(Seurat)
library(BPCells)

# Get module name and directory (needed to access files within the module directory)
module_name = param("module")
module_dir = file.path("modules", module_name)
```

```{r}
#| label: read_data_preparation

# Module directory 'results' contains all output that should be provided together with the final report of the project
dir.create(file.path(module_dir, "results"), showWarnings=FALSE)

# Module directory 'tmp' contains all intermediate files only used for this module
dir.create(file.path(module_dir, "tmp"), showWarnings=FALSE)

# Only if counts are stored on disk: module directory 'counts' contains counts for all datasets and assays
dir.create(file.path(module_dir, "counts"), showWarnings=FALSE)
```

## Overview

In this chapter, we read in counts data as "features x barcodes" matrices where features are typically genes (but can also be peaks, protein antibody, ...) and barcodes are typically cells (but can also be nuclei or spots). If provided, we also read in additional metadata for features and barcodes. For spatial data, we additionally read in the image data. All data is imported into an Seurat object. Next, basic summary stats are calculated. Finally, the Seurat object is saved for further processing in the next chapters.

## Datasets


```{r}
#| label: tbl-read_data_datasets
#| tbl-cap: "Datasets to be read"

# Read table with datasets
datasets_file = param("datasets_file")
datasets = ReadDatasetsTable(datasets_file)

for (i in 1:nrow(datasets)) {
  # Required columns
  assertthat::assert_that(!is.na(datasets$sample[i]),
                            msg=FormatMessage("Column 'sample' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$experiment[i]),
                            msg=FormatMessage("Column 'experiment' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$technology[i]),
                            msg=FormatMessage("Column 'technology' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$assays[i]),
                            msg=FormatMessage("Column 'assays' cannot be empty or NA (row {i})."))
  assertthat::assert_that(!is.na(datasets$path[i]),
                            msg=FormatMessage("Column 'path' cannot be empty or NA (row {i})."))
  
  # Default assay must be present in all datasets - potential fix: Add an empty default assay table for each dataset without default assay - but is it neccessary (?)
  default_assay = param("default_assay")
  assertthat::assert_that(default_assay %in% (strsplit(datasets$assays[i], ",") %>% unlist() %>% trimws()),
                            msg=FormatMessage("Each dataset must contain the default assay {default_assay} (dataset {path})."))
}

# If barcode_suffix is NA, generates suffixes per experiment (-1, -2, ...) 
if (any(is.na(datasets$barcode_suffix))) {
  assertthat::assert_that(sum(is.na(datasets$barcode_suffix)) == nrow(datasets),
                            msg=FormatMessage("If one barcode suffix is empty/NA, all barcode suffixes need to be empty/NA."))
  datasets$barcode_suffix = factor(datasets$experiment, level=unique(datasets$experiment)) %>% as.integer() %>% paste0("-", .)
}

# Print datasets
gt(datasets)
```

Individual stats by technology are:

```{r}
#| label: read_data_metrics

# Read metrics tables into list
metrics_tables = purrr::map(1:nrow(datasets), function(i) {
  experiment = datasets$experiment[i]
  technology = datasets$technology[i]
  metrics_file = datasets$metrics_file[i]
  
  if (!is.na(metrics_file)) {
    metrics_table = ReadMetrics(metrics_file=metrics_file,  technology=technology) %>% 
      dplyr::bind_rows() %>%
      dplyr::mutate(Dataset=experiment, .before=1)
  } else {
    metrics_table = data.frame(Dataset=experiment)
  }
  
  return(metrics_table)
})

# Then group and bind rows by technology
dataset_rows_by_tech = split(1:nrow(datasets), datasets$technology)
metrics_tables_by_tech = purrr::map(dataset_rows_by_tech, function(rows) {
  metrics_table = dplyr::bind_rows(metrics_tables[rows])
  return(metrics_table)
})
```

Individual stats by technology are:

::: panel-tabset
```{r}
#| results: asis

if (interactive()) {
  # In browser:
  for(n in names(metrics_tables_by_tech)) {
    metrics_tables_by_tech[[n]] %>% gt() %>% print()
  }
} else {
  # When calling knit:
  
  # How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
  # String to create a chunk for each assay ({{assay}} is placeholder)
  chunk_template = "

### {{technology}}

\`\`\`{r}
#| label: tbl-read_data_metrics_{{technology}}
#| tbl-cap: 'Metrics for technology {{technology}}'
#| echo: false
#| timeit: null

metrics_tables_by_tech[['{{technology}}']] %>% gt()
\`\`\`

"
  
  chunk_code = purrr::map_chr(names(metrics_tables_by_tech), function(n) {
    knitr::knit_expand(text=chunk_template, technology=n) %>%
      knitr::knit_child(text=., envir=environment(), quiet=TRUE) %>%
      return()
  })
  cat(chunk_code, sep = '\n')
}
```
:::

## Read counts

```{r}
#| label: read_data_read_counts
#| timeit: true

# Read counts

############################
# Read counts for datasets #
############################

counts_lst = list()

for (i in 1:nrow(datasets)) {
  ###############################################################################
  # Get experiment, sample, dataset path, technology, assays and barcode suffix #
  ###############################################################################
  sample = datasets$sample[i]
  experiment = datasets$experiment[i]
  path = datasets$path[i]
  technology = datasets$technology[i]
  assays = strsplit(datasets$assays[i], split=",") %>% unlist() %>% trimws()
  barcode_suffix = datasets$barcode_suffix[i]
  if (is.na(barcode_suffix)) {
    barcode_suffix = paste0("-", i)
  } else if (barcode_suffix=='NO') {
    barcode_suffix = NULL
  }

  #########################
  # Read barcode metadata #
  #########################
  barcode_metadata_files = datasets$barcode_metadata_files[i]
  if (is.na(barcode_metadata_files)) {
    barcode_metadata = NULL
  } else {
    barcode_metadata_files = strsplit(barcode_metadata_files, split=",")  %>% unlist() %>% trimws()
    barcode_metadata = purrr::map(barcode_metadata_files, ReadMetadata)
    if (length(barcode_metadata) == 1) {
      barcode_metadata = barcode_metadata[[1]]
    }
  }
  
  #########################
  # Read feature metadata #
  #########################
  feature_metadata_files = datasets$feature_metadata_files[i]
  if (is.na(feature_metadata_files)) {
    feature_metadata = NULL
  } else {
    feature_metadata_files = strsplit(feature_metadata_files, split=",")  %>% unlist() %>% trimws()
    feature_metadata = purrr::map(feature_metadata_files, ReadMetadata)
    if (length(feature_metadata) == 1) {
      feature_metadata = feature_metadata[[1]]
    }
  }
  
  ###############
  # Read counts #
  ###############
  
  # Read counts
  counts_lst[[i]] = ReadCounts(path=path,
                                    technology=technology,
                                    assays=assays,
                                    barcode_metadata=barcode_metadata,
                                    feature_metadata=feature_metadata,
                                    barcode_suffix=barcode_suffix)
  
  ########################################
  # If requested, how to rename features #
  ########################################
  
  # - use NA or 'NO' for no renaming
  # - use 'ENSEMBL' to fetch gene symbols and information
  # - use a feature metadata column
  # - can be specified for all assays or for each assay separately

  rename_features = datasets$rename_features[i]
  if (is.na(rename_features) | rename_features=='NO') {
    # No renaming
    rename_features = 'NO'
  }
  
  if (rename_features != 'NO' ) {
    # Rename using a feature metadata column or using Ensembl (keyword: "ENSEMBL")
    rename_features = strsplit(rename_features, split=",")  %>% unlist() %>% trimws()
    assertthat::assert_that(length(rename_features) == 1 | length(rename_features) == length(counts_lst[[i]]),
                            msg=FormatMessage("When renaming features, either specify one feature metadata column/ENSEMBL to be used for all assays or specify a feature metadata column/ENSEMBL for each assay (dataset {path})."))
  }
  
  # Make sure there an option for each assay
  if (length(rename_features) == 1) {
      rename_features = rep(rename_features, length(counts_lst[[i]]))
  }
  
  # Now loop through assays
  for (j in seq_along(counts_lst[[i]])) {
    assay = attr(counts_lst[[i]][[j]], "assay")
    
    # if 'NO', no renaming
    if (rename_features[j] == 'NO') {
      next
    }
  
    # If 'ENSEMBL', get ensembl information for features
    if (rename_features[j] == "ENSEMBL") {
      # Query Ensembl
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      ensembl_annotation = EnsemblFetchGeneInfo(ids=rownames(feature_metadata), 
                                                species=param("species"),
                                                ensembl_version=param("ensembl"),
                                                mart_attributes = c(ensembl_id="ensembl_gene_id", ensembl_symbol="external_gene_name",
                                                                     ensembl_biotype="gene_biotype", ensembl_description="description", 
                                                                     ensembl_chr="chromosome_name",ensembl_start_position="start_position", 
                                                                     ensembl_end_position="end_position", ensembl_strand="strand"),
                                                useCache=TRUE)
      feature_metadata = dplyr::bind_cols(feature_metadata, ensembl_annotation, .name_repair="minimal")
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      
      # Use metadata column 'ensembl_symbol'
      rename_features[j] = "ensembl_symbol"
    }
    
    # Check that metadata column exists
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    feature_name_column = rename_features[j]
    
    is_column_index = suppressWarnings(feature_name_column %>% as.numeric() %>% is.na() %>% not())
    if (is_column_index) {feature_name_column = as.numeric(feature_name_column)}
    
    if (is.numeric(feature_name_column)) {
        assertthat::assert_that(feature_name_column <= ncol(feature_metadata),
                                msg=FormatMessage("Column number {feature_name_column} exceeds the number of columns in the feature metadata for dataset {path}, assay {assay}."))
    } else {
        assertthat::assert_that(feature_name_column %in% colnames(feature_metadata),
                                msg=FormatMessage("Column {feature_name_column} cannot be found in the feature metadata available for dataset {path}, assay {assay}."))
    }
    
    # Now make new feature names Seurat-compatible and unique
    new_feature_names = feature_metadata[, feature_name_column, drop=TRUE]
    new_feature_names = ifelse(is.na(new_feature_names), rownames(feature_metadata), new_feature_names)
    if (any(grepl(pattern="_", x=new_feature_names, fixed=TRUE))) {
      warning(FormatMessage("New feature names contain '_' after renaming for dataset {path}, assay {assay}. All occurences will be replaced with '-'."))
      new_feature_names = gsub(pattern="_", replacement="-", x=new_feature_names, fixed=TRUE)
    }
    if (any(duplicated(new_feature_names))) {
      warning(FormatMessage("New features contains duplicate values after renaming for dataset {path}, assay {assay}. Feature names will be made unique."))
      new_feature_names = make.unique(new_feature_names)
    }
    
    # Rename
    rownames(counts_lst[[i]][[j]]) = new_feature_names
    rownames(feature_metadata) = new_feature_names
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
  }
    
  ##################################################
  # If requested, write counts to matrix directory #
  ##################################################
  
  # - allows to analyse big datasets
  # - requires Seurat v5 and BPcells
  # - else convert to in-memory matrix of type dgCMatrix  
  
  on_disk_counts = param("on_disk_counts")
  
  for(j in seq_along(counts_lst[[i]])) {
     # Keep attributes barcode_metadata and feature_metadata
    barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
    feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
    assay = attr(counts_lst[[i]][[j]], "assay")

    if (on_disk_counts) {
      # Write counts to matrix directory
      WriteCounts(counts=counts_lst[[i]][[j]], 
                  path=file.path(module_dir, "counts", paste(experiment, assay, sep=".")), 
                  format="matrix_directory", 
                  overwrite=param("on_disk_counts_overwrite"))
      
      # Then open matrix directory for analysis
      counts_lst[[i]][[j]] = BPCells::open_matrix_dir(file.path(module_dir, "counts", paste(experiment, assay, sep=".")))
    } else {
      # Convert to in-memory matrix
      counts_lst[[i]][[j]] = as(counts_lst[[i]][[j]], "dgCMatrix")
    }
    
    # Add attributes barcode_metadata and feature_metadata, as well as technology and assay
    attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
    attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
    attr(counts_lst[[i]][[j]], "technology") = technology
    attr(counts_lst[[i]][[j]], "assay") = assay
    attr(counts_lst[[i]][[j]], "sample") = sample
    attr(counts_lst[[i]][[j]], "orig.ident") = experiment
    attr(counts_lst[[i]][[j]], "path") = path
  }
}
```

```{r}
#| label: tbl-read_data_counts_summary
#| tbl-cap: "Counts for each dataset"

# Print summary of feature and barcodes
counts_summary = data.frame(
  sample = purrr::map_depth(counts_lst, 2, attr, "sample") %>% purrr::flatten() %>% unlist(),
  dataset = purrr::map_depth(counts_lst, 2, attr, "orig.ident") %>% purrr::flatten() %>% unlist(),
  assay = purrr::map_depth(counts_lst, 2, attr, "assay") %>% purrr::flatten() %>% unlist(),
  barcodes = purrr::map_depth(counts_lst, 2, ncol) %>% purrr::flatten() %>% unlist(),
  features = purrr::map_depth(counts_lst, 2, nrow) %>% purrr::flatten() %>% unlist())

gt(counts_summary)
```

Downsample here

```{r}
#| label: read_data_downsample

# Downsample barcodes

# Determine number of barcodes to downsample
downsample_barcodes_n = param("downsample_barcodes_n")
downsample_barcodes_equally = param("downsample_barcodes_equally")

if (!is.null(downsample_barcodes_n) | downsample_barcodes_equally) {
  n = downsample_barcodes_n
  
  # Get barcodes per dataset
  barcodes_per_dataset = purrr::map(counts_lst, function(cts) {
    purrr::map(cts, colnames) %>% 
      unlist %>%
      unique() %>%
      return()
  })
  
  # If downsample equally, update sample number to number of barcodes of the smallest sample
  if (downsample_barcodes_equally) {
    n = purrr::map(barcodes_per_dataset, length) %>% 
      unlist() %>% 
      min()
  }
  
  # Downsample barcodes
  barcodes_per_dataset = purrr::map(barcodes_per_dataset, function(b) {
    set.seed(getOption("random_seed"))
    b = sample(b, n)
    return(b)
  })
  
  # Subset counts and metadata
  for (i in seq(counts_lst)) {
    for(j in seq_along(counts_lst[[i]])) {
      # Metadata gets lost when subsetting
      feature_metadata = attr(counts_lst[[i]][[j]], "feature_metadata")
      barcode_metadata = attr(counts_lst[[i]][[j]], "barcode_metadata")
      technology = attr(counts_lst[[i]][[j]], "technology")
      assay = attr(counts_lst[[i]][[j]], "assay")
      sample = attr(counts_lst[[i]][[j]], "sample")
      experiment = attr(counts_lst[[i]][[j]], "orig.ident")
      path = attr(counts_lst[[i]][[j]], "path")
      
      # Subset counts
      keep = colnames(counts_lst[[i]][[j]]) %in% barcodes_per_dataset[[i]]
      counts_lst[[i]][[j]] = counts_lst[[i]][[j]][, keep]
      
      # Subset barcode metadata
      keep = rownames(barcode_metadata) %in% barcodes_per_dataset[[i]]
      barcode_metadata = barcode_metadata[keep,]

      # Restore metadata
      attr(counts_lst[[i]][[j]], "feature_metadata") = feature_metadata
      attr(counts_lst[[i]][[j]], "barcode_metadata") = barcode_metadata
      attr(counts_lst[[i]][[j]], "technology") = technology
      attr(counts_lst[[i]][[j]], "assay") = assay
      attr(counts_lst[[i]][[j]], "sample") = sample
      attr(counts_lst[[i]][[j]], "orig.ident") = experiment
      attr(counts_lst[[i]][[j]], "path") = path
    }
  }
    
  # Message
  knitr::asis_output("\n::: {.callout-note}\nDownsampled!\n:::\n")
}
```

## Create Seurat object

Create object

```{r}
#| label: read_data_seurat_object
#| timeit: true

# List all assays in all datasets
all_assays = purrr::map_depth(counts_lst, 2, attr, "assay") %>% 
  purrr::flatten() %>% 
  unlist() %>% 
  unique()

#################
# Default assay #
#################

# Collect default assay datasets
default_assay = param("default_assay")
has_assay = purrr::map(counts_lst, purrr::pluck_exists, default_assay) %>% purrr::flatten_lgl()
assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, default_assay)
names(assay_counts_lst) = purrr::map(assay_counts_lst, attr, "orig.ident") %>% unlist()

# Create Seurat default assay object
assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
rownames(assay_obj@meta.data) = rownames(assay_obj)

# Collect feature metadata for default assay datasets
feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
  metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
  return(metadata)
}) %>% dplyr::bind_rows()

# Group by feature name and make unique values
feature_metadata = feature_metadata %>% 
  dplyr::group_by(rowname) %>%
  dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
  as.data.frame()
rownames(feature_metadata) = feature_metadata$rowname
feature_metadata = feature_metadata %>% dplyr::select(-rowname)

# Add to Seurat default assay object
assay_obj = AddFeatureMetadata(assay_obj, metadata=feature_metadata)

# Collect barcode metadata (from all assays/datasets)
barcode_metadata = purrr::map_dfr(seq_along(counts_lst), function(i) {
  metadata = data.frame(rowname=colnames(assay_counts_lst[[i]]))
  metadata$orig.ident = attr(assay_counts_lst[[i]], "orig.ident")
  metadata$sample = attr(assay_counts_lst[[i]], "sample")
  metadata$technology = attr(assay_counts_lst[[i]], "technology")
  
  bc_meta = attr(assay_counts_lst[[i]], "barcode_metadata") %>% 
    tibble::rownames_to_column()
  bc_meta$sample = NULL
  bc_meta$technology = NULL
  bc_meta$orig.ident = NULL
  metadata = dplyr::left_join(metadata, bc_meta, by="rowname")
  rownames(metadata) = metadata$rowname
  
  metadata = metadata %>% dplyr::select(-rowname)
  return(metadata)
})
barcode_metadata$orig.ident = factor(barcode_metadata$orig.ident, levels=unique(barcode_metadata$orig.ident))
barcode_metadata$sample = factor(barcode_metadata$sample, levels=unique(barcode_metadata$sample))
barcode_metadata$technology = factor(barcode_metadata$technology, levels=unique(barcode_metadata$technology))

# Create Seurat object
sc = Seurat::CreateSeuratObject(counts=assay_obj, meta.data=barcode_metadata, assay=default_assay, names.delim=NULL, names.field=NULL)

################
# Other assays #
################

# Add other assays to Seurat object
for(a in setdiff(all_assays, default_assay)) {
  # Collect assay datasets
  has_assay = purrr::map(counts_lst, purrr::pluck_exists, a) %>% purrr::flatten_lgl()
  assay_counts_lst = purrr::map(counts_lst[has_assay], purrr::pluck, a)
  
  # Create Seurat default assay object
  assay_obj = SeuratObject::CreateAssay5Object(counts=assay_counts_lst)
  rownames(assay_obj@meta.data) = rownames(assay_obj)
  
  # If there is layer (sample) information in the default assay, also use layer information in other assays
  if (!identical(SeuratObject::Layers(sc[[default_assay]]), c("counts")) & 
      identical(SeuratObject::Layers(assay_obj), c("counts"))) {
    assay_obj = split(assay_obj, sc[[]][colnames(assay_obj), "orig.ident"])
  }

  # Collect feature metadata for default assay datasets
  feature_metadata = purrr::map(seq_along(assay_counts_lst), function(i) {
    metadata = attr(assay_counts_lst[[i]], "feature_metadata") %>% tibble::rownames_to_column()
    return(metadata)
  }) %>% dplyr::bind_rows()

  # Group by feature name and make unique values
  feature_metadata = feature_metadata %>% 
    dplyr::group_by(rowname) %>%
    dplyr::summarise_all(dplyr::first, na_rm=TRUE) %>% 
    as.data.frame()
  rownames(feature_metadata) = feature_metadata$rowname
  feature_metadata = feature_metadata %>% dplyr::select(-rowname)

  # Add to Seurat default assay object
  assay_obj = AddFeatureMetadata(assay_obj, metadata=feature_metadata)
  sc[[a]] = assay_obj
}
```

Add images

```{r}
#| label: read_data_read_images
#| timeit: true

# Add images (10x Visium and 10x Xenium only)
for (i in seq_along(counts_lst)) {
  i = 1
  technology = attr(counts_lst[[i]][[1]], "technology")
  orig_ident = attr(counts_lst[[i]][[1]], "orig.ident")
  path = attr(counts_lst[[i]][[1]], "path")
  default_assay = param("default_assay")
  
  # Two technologies have image data
  if (technology == "10x_visium") {
    # 10x Visium: image is in subdirectory 'spatial'
    image_dir = file.path(dirname(path), "spatial")
    assertthat::assert_that(dir.exists(image_dir),
                                msg=FormatMessage("10x Visium dataset {path} does not have a directory 'spatial' with spatial data."))
  } else if (technology == "10x_xenium") {
    # 10x Xenium_ image is in the same directory
    image_dir = dirname(path)
  } else {
    # other: skip
    next
  }
  
  # For renaming and reordering of barcodes in image
  barcodes = sc[[]] %>%
    dplyr::filter(orig.ident==orig_ident) %>%
    dplyr::select(orig_barcode)
  barcodes = setNames(rownames(barcodes), barcodes$orig_barcode)
  j = match(barcodes, Cells(sc))
  barcodes = barcodes[order(j)]
  
  next
  
  # Read image
  image = ReadImage(image_dir=image_dir, 
                    technology=technology, 
                    assay=default_assay, 
                    barcodes=barcodes,
                    type="centroids")
  
  # Add image (10x visium) or field of view (10x xenium)
  if (technology == "10x_visium") {
    key = "image"
  } else if (technology == "10x_xenium") {
    key = "fov"
  }
  if (nrow(datasets) > 1) {
    key = paste0(key, "." , orig_ident)
  }
  image@key = paste0(gsub("[^[:alnum:]]", "", key), "_")
  sc[[key]] = image
  
  # If additional barcode metadata is provided, add as well
  barcode_metadata = attr(image, "barcode_metadata")
  if (!is.null(barcode_metadata)) {
    sc = Seurat::AddMetaData(sc, barcode_metadata)
  }
  
}
```

```{r}
#| label: read_data_basic_stats

############
# Barcodes #
############

# Calculate for each assay
for (a in Seurat::Assays(sc)) {

  m = paste("pCountsTop50", a, sep="_")
  sc[[m]] = NA
  
  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]])) {
    
    # Get data for layer (sample)
    counts = sc[[a]][[l]]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]

    # Split columns into chunks
    col_indices = 1:ncol(counts)
    chunk_size = 5000
    chunks = split(col_indices, ceiling(seq_along(col_indices)/chunk_size)) 
    
    # Now calculate a matrix that for each column there is a TRUE when the gene is in the top 50 otherwise FALSE
    top50_counts = purrr::map(chunks, function(c) {
      r = rowSums(counts[, c]) > 0
      cts = as(counts[r, c], "dgCMatrix")
      n = diff(cts@p)  ## number of non-zeros per column
      col_lst = split(as.integer(cts@x)*(-1), rep.int(1:ncol(cts), n))  ## columns to list
      col_lst = lapply(col_lst, function(x) return(sort(x)*(-1)))
      top50_cts = sapply(col_lst, function(x) return(sum(head(x, 50))))
      return(top50_cts)
    }, .progress=TRUE) %>% purrr::flatten_int()
    
    # Zero counts barcodes will not be in top50_counts - add them here
    top50_counts = ifelse(total_counts==0, 0, top50_counts)
    top50_perc = ifelse(total_counts>0, top50_counts/total_counts*100, NA)
    
    # Then calculate percentage top50
    top50_perc = setNames(top50_counts, bcs)
    m = paste("pCountsTop50", a, sep="_")
    sc[[m]][names(top50_perc), ] = top50_perc
  }
}


############
# Features #
############

# Calculate for each assay
for (a in Seurat::Assays(sc)) {
  # And each layer (dataset)
  for (l in SeuratObject::Layers(sc[[a]])) {
    # Get data for layer (sample)
    counts = sc[[a]][[l]]
    bcs = counts %>% colnames()
    total_counts = sc[[]][bcs, paste0("nCount_", a), drop=TRUE]
    counts_perc = t(t(counts)/total_counts)*100
    
    # Calculate feature filters
    num_bcs_expr = rowSums(counts >= 1)
    num_bcs_expr_threshold = rowSums(counts >= 1)
    
    # Calculate mean of counts_perc per gene
    row_indices = 1:nrow(counts_perc)
    chunk_size = 1000
    chunks = split(row_indices, ceiling(seq_along(row_indices)/chunk_size)) 
    
    counts_median = purrr::map(chunks, function(c) {
      counts_perc[c, ] %>% 
        as("dgCMatrix") %>% 
        sparseMatrixStats::rowMedians() %>%
        return()
    }, .progress=TRUE) %>% purrr::flatten_dbl()
    counts_median = setNames(counts_median, rownames(counts_perc))
    
    # Add to metadata
    if (l=="counts") {
      sc[[a]]["nBcs"] = num_bcs_expr
      sc[[a]]["nBcsThreshold"] = num_bcs_expr_threshold
      sc[[a]]["meanPerc"] = counts_median
    } else {
      n = gsub(x=l, pattern="counts\\.", replacement="")
      sc[[a]][paste("nBcs", n, sep="_")] = num_bcs_expr
      sc[[a]][paste("nBcsThreshold", n, sep="_")] = num_bcs_expr_threshold
      sc[[a]][paste("meanPerc", n, sep="_")] = counts_median
    }
  }
}
```

The following table shows available metadata (columns) of the first five barcodes (rows). These metadata provide additional information such as:

-   the dataset it belongs to ("orig.ident")
-   the sample it belongs to
-   the technology with which it was produced
-   its original name in the dataset
-   additional metadata provided together with it
-   the total number of counts and features for each assay

```{r}
#| label: tbl-read_data_barcode_metadata
#| tbl-cap: "Barcode metadata"

df = sc[[]] %>% head(5)
gt(df, rownames_to_stub=TRUE)
```

The next table shows available metadata (columns) of the first five features (rows) for each assay.

::: panel-tabset
```{r}
#| results: asis

if (interactive()) {
  # In browser:
  for(a in Seurat::Assays(sc)) {
    sc[[a]] %>% head(5) %>% gt(rownames_to_stub=TRUE) %>% print()
  }
} else {
  # When calling knit:
  
  # How to dynamically generate tables: https://stackoverflow.com/questions/73585417/iterating-to-create-tabs-with-gt-in-quarto
  # String to create a chunk for each assay ({{assay}} is placeholder)
  chunk_template = "

### {{assay}}

\`\`\`{r}
#| label: tbl-read_data_feature_metadata_{{assay}}
#| tbl-cap: 'Feature metadata for assay {{assay}}'
#| echo: false
#| timeit: null

sc[['{{assay}}']] %>% head(5) %>% gt(rownames_to_stub=TRUE)
\`\`\`

"
  
  chunk_code = purrr::map_chr(Seurat::Assays(sc), function(a) {
    knitr::knit_expand(text=chunk_template, assay=a) %>%
      knitr::knit_child(text=., envir=environment(), quiet=TRUE) %>%
      return()
  })
  cat(chunk_code, sep = '\n')
}
```
:::

## Basic summary

```{r}
#s

```

## Save Seurat object

```{r}
#| label: read_data_save_object
#| timeit: true

saveRDS(object=sc, file=file.path(module_dir, "sc.rds"))
```
